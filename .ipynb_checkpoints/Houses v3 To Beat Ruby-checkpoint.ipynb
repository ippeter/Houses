{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression, LinearRegression, SGDRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load House train data\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Put the labels aside\n",
    "labels_orig = train.SalePrice.as_matrix().reshape(-1, 1)\n",
    "\n",
    "# Which columns have NaN values?\n",
    "count = 0\n",
    "\n",
    "#for col in train.columns:\n",
    "#    if(train[col].isnull().sum() > 0):\n",
    "#        print(col)\n",
    "#        count = count + 1\n",
    "        \n",
    "#print(\"Total\", count, \"columns with NaNs\")\n",
    "\n",
    "# Define variables\n",
    "cols = []\n",
    "cols_count = 0\n",
    "\n",
    "# Handling years as categorial \n",
    "use_fe_2 = False\n",
    "\n",
    "# Using kind of a total sum of square feet\n",
    "use_fe_3 = True\n",
    "\n",
    "# Using new feature of Clustering\n",
    "use_fe_4 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour  \n",
       "0         Lvl  \n",
       "1         Lvl  \n",
       "2         Lvl  \n",
       "3         Lvl  \n",
       "4         Lvl  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, :9].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSSubClass\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "features = enc.fit_transform(train.MSSubClass.values.reshape(-1, 1))\n",
    "df_train = pd.get_dummies(train.MSSubClass, prefix=\"MSSubClass\")\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MSZoning\n",
    "#train.MSZoning.fillna(\"RM\", inplace=True)\n",
    "train.MSZoning.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.MSZoning).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.MSZoning, prefix=\"MSZoning\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotFrontage\n",
    "train.LotFrontage.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.LotFrontage.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.LotFrontage], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotArea\n",
    "features = np.concatenate( [features, train.LotArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.LotArea], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Street\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Street).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Street, prefix=\"Street\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Alley\n",
    "train.Alley.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Alley).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Alley, prefix=\"Alley\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotShape\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LotShape).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.LotShape, prefix=\"LotShape\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LandContour\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LandContour).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.LandContour, prefix=\"LandContour\")], axis=1)\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType  \\\n",
       "0    AllPub    Inside       Gtl      CollgCr       Norm       Norm     1Fam   \n",
       "1    AllPub       FR2       Gtl      Veenker      Feedr       Norm     1Fam   \n",
       "2    AllPub    Inside       Gtl      CollgCr       Norm       Norm     1Fam   \n",
       "3    AllPub    Corner       Gtl      Crawfor       Norm       Norm     1Fam   \n",
       "4    AllPub       FR2       Gtl      NoRidge       Norm       Norm     1Fam   \n",
       "\n",
       "  HouseStyle  OverallQual  OverallCond  \n",
       "0     2Story            7            5  \n",
       "1     1Story            6            8  \n",
       "2     2Story            7            5  \n",
       "3     2Story            7            5  \n",
       "4     2Story            8            5  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 9:19].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "#train.Utilities.fillna(\"AllPub\", inplace=True)\n",
    "train.Utilities.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Utilities).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Utilities, prefix=\"Utilities\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotConfig\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LotConfig).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.LotConfig, prefix=\"LotConfig\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LandSlope\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LandSlope).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.LandSlope, prefix=\"LandSlope\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Neighborhood \n",
    "features = np.concatenate( [features, pd.get_dummies(train.Neighborhood).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Neighborhood, prefix=\"Neighborhood\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Condition1\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Condition1).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Condition1, prefix=\"Condition1\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Condition2\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Condition2).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Condition2, prefix=\"Condition2\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BldgType\n",
    "features = np.concatenate( [features, pd.get_dummies(train.BldgType).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.BldgType, prefix=\"BldgType\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# HouseStyle\n",
    "features = np.concatenate( [features, pd.get_dummies(train.HouseStyle).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.HouseStyle, prefix=\"HouseStyle\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# OverallQual\n",
    "#enc = OneHotEncoder(sparse=False)\n",
    "#features = np.concatenate( [features, enc.fit_transform(train.OverallQual.values.reshape(-1, 1))], axis=1 )\n",
    "features = np.concatenate( [features, train.OverallQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.OverallQual], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# OverallCond\n",
    "#enc = OneHotEncoder(sparse=False)\n",
    "#features = np.concatenate( [features, enc.fit_transform(train.OverallCond.values.reshape(-1, 1))], axis=1 )\n",
    "features = np.concatenate( [features, train.OverallCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.OverallCond], axis=1)\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Wd Sdng</td>\n",
       "      <td>Wd Shng</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearBuilt  YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd  \\\n",
       "0       2003          2003     Gable  CompShg     VinylSd     VinylSd   \n",
       "1       1976          1976     Gable  CompShg     MetalSd     MetalSd   \n",
       "2       2001          2002     Gable  CompShg     VinylSd     VinylSd   \n",
       "3       1915          1970     Gable  CompShg     Wd Sdng     Wd Shng   \n",
       "4       2000          2000     Gable  CompShg     VinylSd     VinylSd   \n",
       "\n",
       "  MasVnrType  MasVnrArea ExterQual ExterCond  \n",
       "0    BrkFace       196.0        Gd        TA  \n",
       "1       None         0.0        TA        TA  \n",
       "2    BrkFace       162.0        Gd        TA  \n",
       "3       None         0.0        TA        TA  \n",
       "4    BrkFace       350.0        Gd        TA  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 19:29].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YearBuilt\n",
    "#if (use_fe_2):\n",
    "#    features = np.concatenate( [features, pd.get_dummies(train.YearBuilt).as_matrix()], axis=1 )\n",
    "#else:\n",
    "#    features = np.concatenate( [features, train.YearBuilt.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "train['HouseAge'] = train.YrSold - train.YearBuilt\n",
    "features = np.concatenate( [features, train.HouseAge.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.HouseAge], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# YearRemodAdd\n",
    "#if (False):\n",
    "#    features = np.concatenate( [features, pd.get_dummies(train.YearRemodAdd).as_matrix()], axis=1 )\n",
    "#else:\n",
    "#    features = np.concatenate( [features, train.YearRemodAdd.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "train['AgeSinceRemod'] = train.YrSold - train.YearRemodAdd\n",
    "features = np.concatenate( [features, train.AgeSinceRemod.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.AgeSinceRemod], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# RoofStyle\n",
    "features = np.concatenate( [features, pd.get_dummies(train.RoofStyle).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.RoofStyle, prefix=\"RoofStyle\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# RoofMatl\n",
    "features = np.concatenate( [features, pd.get_dummies(train.RoofMatl).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.RoofMatl, prefix=\"RoofMatl\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Exterior1st\n",
    "#train.Exterior1st.fillna(\"Wd Sdng\", inplace=True)\n",
    "train.Exterior1st.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Exterior1st).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Exterior1st, prefix=\"Exterior1st\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Exterior2nd\n",
    "#train.Exterior2nd.fillna(\"Wd Sdng\", inplace=True)\n",
    "train.Exterior2nd.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Exterior2nd).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Exterior2nd, prefix=\"Exterior2nd\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MasVnrType\n",
    "#train.MasVnrType.fillna(\"None\", inplace=True)\n",
    "train.MasVnrType.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.MasVnrType).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.MasVnrType, prefix=\"MasVnrType\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MasVnrArea\n",
    "train.MasVnrArea.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.MasVnrArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.MasVnrArea], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# ExterQual\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0}\n",
    "train[\"ExterQual\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.ExterQual).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.ExterQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.ExterQual], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# ExterCond\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0}\n",
    "train[\"ExterCond\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.ExterCond).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.ExterCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.ExterCond], axis=1)\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Foundation</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>706</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBlock</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>978</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>486</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BrkTil</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>216</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Av</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>655</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1  BsmtFinSF1  \\\n",
       "0      PConc       Gd       TA           No          GLQ         706   \n",
       "1     CBlock       Gd       TA           Gd          ALQ         978   \n",
       "2      PConc       Gd       TA           Mn          GLQ         486   \n",
       "3     BrkTil       TA       Gd           No          ALQ         216   \n",
       "4      PConc       Gd       TA           Av          GLQ         655   \n",
       "\n",
       "  BsmtFinType2  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  \n",
       "0          Unf           0        150          856  \n",
       "1          Unf           0        284         1262  \n",
       "2          Unf           0        434          920  \n",
       "3          Unf           0        540          756  \n",
       "4          Unf           0        490         1145  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 29:39].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Foundation\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Foundation).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Foundation, prefix=\"Foundation\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtQual\n",
    "train.BsmtQual.fillna(\"NA\", inplace=True)\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtQual\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtQual).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtQual], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtCond\n",
    "train.BsmtCond.fillna(\"NA\", inplace=True)\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtCond\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtCond).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtCond], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtExposure\n",
    "train.BsmtExposure.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.BsmtExposure).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.BsmtExposure, prefix=\"BsmtExposure\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinType1\n",
    "train.BsmtFinType1.fillna(\"NA\", inplace=True)\n",
    "di = {\"GLQ\": 6.0, \"ALQ\": 5.0, \"BLQ\": 4.0, \"Rec\": 3.0, \"LwQ\": 2.0, \"Unf\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtFinType1\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtFinType1).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtFinType1.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtFinType1], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinSF1\n",
    "train.BsmtFinSF1.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtFinSF1.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtFinSF1], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinType2\n",
    "train.BsmtFinType2.fillna(\"NA\", inplace=True)\n",
    "di = {\"GLQ\": 6.0, \"ALQ\": 5.0, \"BLQ\": 4.0, \"Rec\": 3.0, \"LwQ\": 2.0, \"Unf\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtFinType2\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtFinType2).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtFinType2.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtFinType2], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinSF2\n",
    "train.BsmtFinSF2.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtFinSF2.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtFinSF2], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtUnfSF\n",
    "train.BsmtUnfSF.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtUnfSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtUnfSF], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# TotalBsmtSF\n",
    "train.TotalBsmtSF.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.TotalBsmtSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.TotalBsmtSF], axis=1)\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heating</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Electrical</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Heating HeatingQC CentralAir Electrical  1stFlrSF  2ndFlrSF  LowQualFinSF  \\\n",
       "0    GasA        Ex          Y      SBrkr       856       854             0   \n",
       "1    GasA        Ex          Y      SBrkr      1262         0             0   \n",
       "2    GasA        Ex          Y      SBrkr       920       866             0   \n",
       "3    GasA        Gd          Y      SBrkr       961       756             0   \n",
       "4    GasA        Ex          Y      SBrkr      1145      1053             0   \n",
       "\n",
       "   GrLivArea  BsmtFullBath  BsmtHalfBath  \n",
       "0       1710             1             0  \n",
       "1       1262             0             1  \n",
       "2       1786             1             0  \n",
       "3       1717             1             0  \n",
       "4       2198             1             0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 39:49].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Heating\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Heating).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Heating, prefix=\"Heating\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# HeatingQC\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.HeatingQC).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0}\n",
    "train[\"HeatingQC\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.HeatingQC.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.HeatingQC], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# CentralAir\n",
    "features = np.concatenate( [features, pd.get_dummies(train.CentralAir).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.CentralAir, prefix=\"CentralAir\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Electrical\n",
    "train.Electrical.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Electrical).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Electrical, prefix=\"Electrical\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# 1stFlrSF\n",
    "features = np.concatenate( [features, train['1stFlrSF'].as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train['1stFlrSF']], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# 2ndFlrSF\n",
    "features = np.concatenate( [features, train['2ndFlrSF'].as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train['2ndFlrSF']], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LowQualFinSF\n",
    "features = np.concatenate( [features, train.LowQualFinSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.LowQualFinSF], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GrLivArea\n",
    "features = np.concatenate( [features, train.GrLivArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.GrLivArea], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFullBath\n",
    "train.BsmtFullBath.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtFullBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtFullBath], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtHalfBath\n",
    "train.BsmtHalfBath.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtHalfBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BsmtHalfBath], axis=1)\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Functional</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>8</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>7</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>9</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FullBath  HalfBath  BedroomAbvGr  KitchenAbvGr KitchenQual  TotRmsAbvGrd  \\\n",
       "0         2         1             3             1          Gd             8   \n",
       "1         2         0             3             1          TA             6   \n",
       "2         2         1             3             1          Gd             6   \n",
       "3         1         0             3             1          Gd             7   \n",
       "4         2         1             4             1          Gd             9   \n",
       "\n",
       "  Functional  Fireplaces FireplaceQu GarageType  \n",
       "0        Typ           0         NaN     Attchd  \n",
       "1        Typ           1          TA     Attchd  \n",
       "2        Typ           1          TA     Attchd  \n",
       "3        Typ           1          Gd     Detchd  \n",
       "4        Typ           1          TA     Attchd  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 49:59].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FullBath\n",
    "features = np.concatenate( [features, train.FullBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.FullBath], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# HalfBath\n",
    "features = np.concatenate( [features, train.HalfBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.HalfBath], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BedroomAbvGr\n",
    "features = np.concatenate( [features, train.BedroomAbvGr.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.BedroomAbvGr], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# KitchenAbvGr\n",
    "features = np.concatenate( [features, train.KitchenAbvGr.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.KitchenAbvGr], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# KitchenQual\n",
    "#train.KitchenQual.fillna(\"TA\", inplace=True)\n",
    "train.KitchenQual.fillna(\"Unknown\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.KitchenQual).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"Unknown\": 0.0}\n",
    "train[\"KitchenQual\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.KitchenQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.KitchenQual], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# TotRmsAbvGrd\n",
    "features = np.concatenate( [features, train.TotRmsAbvGrd.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.TotRmsAbvGrd], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Functional\n",
    "#train.Functional.fillna(\"Typ\", inplace=True)\n",
    "train.Functional.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Functional).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Functional, prefix=\"Functional\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Fireplaces\n",
    "features = np.concatenate( [features, train.Fireplaces.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.Fireplaces], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# FireplaceQu\n",
    "train.FireplaceQu.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.FireplaceQu).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.FireplaceQu, prefix=\"Functional\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageType\n",
    "train.GarageType.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.GarageType).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.GarageType, prefix=\"Functional\")], axis=1)\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1976.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GarageYrBlt GarageFinish  GarageCars  GarageArea GarageQual GarageCond  \\\n",
       "0       2003.0          RFn           2         548         TA         TA   \n",
       "1       1976.0          RFn           2         460         TA         TA   \n",
       "2       2001.0          RFn           2         608         TA         TA   \n",
       "3       1998.0          Unf           3         642         TA         TA   \n",
       "4       2000.0          RFn           3         836         TA         TA   \n",
       "\n",
       "  PavedDrive  WoodDeckSF  OpenPorchSF  EnclosedPorch  \n",
       "0          Y           0           61              0  \n",
       "1          Y         298            0              0  \n",
       "2          Y           0           42              0  \n",
       "3          Y           0           35            272  \n",
       "4          Y         192           84              0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 59:69].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GarageYrBlt\n",
    "train.GarageYrBlt.fillna(train.YearBuilt, inplace=True)\n",
    "\n",
    "#if (False):\n",
    "#    features = np.concatenate( [features, pd.get_dummies(train.GarageYrBlt).as_matrix()], axis=1 )\n",
    "#else:\n",
    "#    features = np.concatenate( [features, train.GarageYrBlt.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "\n",
    "train['GarageAge'] = train.YrSold - train.GarageYrBlt\n",
    "features = np.concatenate( [features, train.GarageAge.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.GarageAge], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageFinish\n",
    "train.GarageFinish.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.GarageFinish).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.GarageFinish, prefix=\"GarageFinish\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageCars\n",
    "train.GarageCars.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageCars.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.GarageCars], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageArea\n",
    "train.GarageArea.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.GarageArea], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageQual\n",
    "train.GarageQual.fillna(\"NA\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.GarageQual).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"GarageQual\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.GarageQual], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageCond\n",
    "train.GarageCond.fillna(\"NA\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.GarageCond).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"GarageCond\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.GarageCond], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# PavedDrive\n",
    "features = np.concatenate( [features, pd.get_dummies(train.PavedDrive).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.PavedDrive, prefix=\"PavedDrive\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# WoodDeckSF\n",
    "features = np.concatenate( [features, train.WoodDeckSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.WoodDeckSF], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# OpenPorchSF\n",
    "features = np.concatenate( [features, train.OpenPorchSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.OpenPorchSF], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# EnclosedPorch\n",
    "features = np.concatenate( [features, train.EnclosedPorch.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.EnclosedPorch], axis=1)\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AgeSinceRemod</th>\n",
       "      <th>GarageAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "      <td>91</td>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   3SsnPorch  ScreenPorch  PoolArea PoolQC Fence MiscFeature  MiscVal  MoSold  \\\n",
       "0          0            0         0    NaN   NaN         NaN        0       2   \n",
       "1          0            0         0    NaN   NaN         NaN        0       5   \n",
       "2          0            0         0    NaN   NaN         NaN        0       9   \n",
       "3          0            0         0    NaN   NaN         NaN        0       2   \n",
       "4          0            0         0    NaN   NaN         NaN        0      12   \n",
       "\n",
       "   YrSold SaleType SaleCondition  SalePrice  HouseAge  AgeSinceRemod  \\\n",
       "0    2008       WD        Normal     208500         5              5   \n",
       "1    2007       WD        Normal     181500        31             31   \n",
       "2    2008       WD        Normal     223500         7              6   \n",
       "3    2006       WD       Abnorml     140000        91             36   \n",
       "4    2008       WD        Normal     250000         8              8   \n",
       "\n",
       "   GarageAge  \n",
       "0        5.0  \n",
       "1       31.0  \n",
       "2        7.0  \n",
       "3        8.0  \n",
       "4        8.0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 69:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3SsnPorch\n",
    "features = np.concatenate( [features, train['3SsnPorch'].as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train['3SsnPorch']], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# ScreenPorch\n",
    "features = np.concatenate( [features, train.ScreenPorch.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.ScreenPorch], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# PoolArea\n",
    "features = np.concatenate( [features, train.PoolArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.PoolArea], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# PoolQC\n",
    "train.PoolQC.fillna(\"NA\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.PoolQC).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"PoolQC\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.PoolQC.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "df_train = pd.concat([df_train, train.PoolQC], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Fence\n",
    "train.Fence.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Fence).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.Fence, prefix=\"Fence\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MiscFeature\n",
    "train.MiscFeature.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.MiscFeature).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.MiscFeature, prefix=\"MiscFeature\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MiscVal\n",
    "#features = np.concatenate( [features, train.MiscVal.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "#df_train = pd.concat([df_train, train.MiscVal], axis=1)\n",
    "#cols_count = cols_count + 1\n",
    "\n",
    "# MoSold\n",
    "if (use_fe_2):\n",
    "    features = np.concatenate( [features, pd.get_dummies(train.MoSold).as_matrix()], axis=1 )\n",
    "    df_train = pd.concat([df_train, pd.get_dummies(train.MoSold, prefix=\"MoSold\")], axis=1)\n",
    "else:\n",
    "    features = np.concatenate( [features, train.MoSold.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    df_train = pd.concat([df_train, train.MoSold], axis=1)\n",
    "    \n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# YrSold\n",
    "if (use_fe_2):\n",
    "    features = np.concatenate( [features, pd.get_dummies(train.YrSold).as_matrix()], axis=1 )\n",
    "    df_train = pd.concat([df_train, pd.get_dummies(train.YrSold, prefix=\"YrSold\")], axis=1)\n",
    "else:\n",
    "    features = np.concatenate( [features, train.YrSold.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    df_train = pd.concat([df_train, train.YrSold], axis=1)\n",
    "    \n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# SaleType\n",
    "#train.SaleType.fillna(\"WD\", inplace=True)\n",
    "train.SaleType.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.SaleType).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.SaleType, prefix=\"SaleType\")], axis=1)\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# SaleCondition\n",
    "features = np.concatenate( [features, pd.get_dummies(train.SaleCondition).as_matrix()], axis=1 )\n",
    "df_train = pd.concat([df_train, pd.get_dummies(train.SaleCondition, prefix=\"SaleCondition\")], axis=1)\n",
    "cols_count = cols_count + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823860406285\n"
     ]
    }
   ],
   "source": [
    "# LivingAreaSF\n",
    "if (use_fe_3):\n",
    "    train['LivingAreaSF'] = train['1stFlrSF'] + train['2ndFlrSF'] + train['TotalBsmtSF'] + \\\n",
    "                            train['GarageArea'] + train['MasVnrArea'] + train['WoodDeckSF'] + \\\n",
    "                            train['OpenPorchSF'] + train['3SsnPorch'] + train['ScreenPorch']\n",
    "\n",
    "    print(pearsonr(train['LivingAreaSF'].values.reshape(-1, 1), train.SalePrice.values.reshape(-1, 1))[0][0])\n",
    "    \n",
    "    features = np.concatenate( [features, train.LivingAreaSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    df_train = pd.concat([df_train, train.LivingAreaSF], axis=1)                      \n",
    "    cols_count = cols_count + 1\n",
    "    \n",
    "    train['LandRatio'] = train['LivingAreaSF'] / train['LotArea']\n",
    "    features = np.concatenate( [features, train.LandRatio.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    df_train = pd.concat([df_train, train.LandRatio], axis=1)   \n",
    "    cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "#labels = (train.SalePrice - train.MiscVal).values.reshape(-1, 1)\n",
    "labels = train.SalePrice.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 23 features scaled\n"
     ]
    }
   ],
   "source": [
    "# First scale labels\n",
    "labels = labels.astype(float)\n",
    "labels_max = labels.max()\n",
    "\n",
    "labels = labels / labels_max\n",
    "\n",
    "count = 0\n",
    "    \n",
    "for jj in range(features.shape[1]):\n",
    "    if((features[:, jj] > 25.).sum() > 0):\n",
    "        mx = float(features[:, jj].max())\n",
    "\n",
    "        features[:, jj] = features[:, jj] / mx\n",
    "        count = count + 1\n",
    "        \n",
    "print(\"Total\", count, \"features scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features processed: 80\n",
      "\n",
      "(1022, 272)\n",
      "(1022, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split for train and test sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=23)\n",
    "\n",
    "print(\"Total features processed:\", cols_count)\n",
    "print(\"\")\n",
    "print(features_train.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection  \n",
    "### Baseline\n",
    "\n",
    "GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)  \n",
    "    +  :  \n",
    "\n",
    "#### 0.020075944788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# For Averaging\n",
    "#\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['Target'] = labels_test.reshape(-1)\n",
    "\n",
    "clfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_ada = pd.DataFrame()\n",
    "results_ada['Target'] = labels_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0378903574145\n",
      "{'hidden_layer_sizes': (150, 50, 10)}\n",
      "0.0303985022085\n",
      "Wall time: 8.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def choose_MLP(X, y):\n",
    "    MLP = MLPRegressor(random_state=23)\n",
    "\n",
    "    parameters_grid = {\n",
    "        \"hidden_layer_sizes\": [(150, 100, 50), (150, 30, 20), (150, 50, 50), (150, 50, 10), (150, 75, 25)],\n",
    "        #\"n_estimators\": [50, 100, 150, 200, 250, 300, 350],\n",
    "        #\"max_depth\": [2, 3, 4, 5, 6],\n",
    "        #\"learning_rate\": [0.05, 0.1, 0.5, 1.0]\n",
    "        #\"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "        #\"min_samples_leaf\": [1, 2, 3, 4]\n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(MLP, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_\n",
    "\n",
    "\n",
    "clf = choose_MLP(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0303985022085\n"
     ]
    }
   ],
   "source": [
    "clf = MLPRegressor(hidden_layer_sizes=(150, 50, 10), random_state=23)\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['MLP'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0218094142142\n",
      "{'learning_rate': 0.075, 'max_depth': 4, 'n_estimators': 200}\n",
      "0.0204735668589\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def choose_LGB(X, y):\n",
    "    LGB = lgb.LGBMRegressor(random_state=23, n_jobs=-1)\n",
    "\n",
    "    parameters_grid = {\n",
    "        #\"boosting_type\": [\"gbdt\", \"dart\", \"goss\", \"rf\"],\n",
    "        \"n_estimators\": [50, 100, 150, 200, 250, 300, 350],\n",
    "        \"max_depth\": [2, 3, 4, 5, 6],\n",
    "        \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1],\n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(LGB, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_    \n",
    "\n",
    "\n",
    "clf = choose_LGB(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['LGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0196013179077\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=350, learning_rate=0.7, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "#results_ada['LGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1611,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_GBM(X, y):\n",
    "    GBR = GradientBoostingRegressor(random_state=23)\n",
    "\n",
    "    parameters_grid = {\n",
    "        \"n_estimators\": [250, 300, 350],\n",
    "        \"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "        \"min_samples_leaf\": [1, 2, 3, 4]\n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(GBR, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0222876966681\n",
      "{'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 250}\n",
      "0.0210613365987\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = choose_GBM(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['GBM'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0203621230414\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "#clf = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=3, random_state=23)\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['GBM'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=350, learning_rate=0.7, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results_ada['GBM'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00786508173164 42\n",
      "0.0105238355887\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = []\n",
    "\n",
    "tmp_f = np.array(features)\n",
    "tmp_l = np.array(labels)\n",
    "\n",
    "for i in range(75):\n",
    "    clf = GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "    clf.fit(tmp_f, tmp_l.reshape(-1))\n",
    "    prediction = clf.predict(tmp_f).reshape(-1, 1)\n",
    "\n",
    "    res.append(mean_absolute_error(tmp_l, prediction))\n",
    "    \n",
    "    if (i == 42):\n",
    "        break\n",
    "\n",
    "    tmp1 = np.abs(tmp_l - prediction)\n",
    "    d = tmp1.argmax()\n",
    "\n",
    "    tmp_f = np.delete(tmp_f, d, 0)\n",
    "    tmp_l = np.delete(tmp_l, d, 0)\n",
    "    \n",
    "print(min(res), argmin(res))\n",
    "\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0174589900129\n"
     ]
    }
   ],
   "source": [
    "f_train, f_test, l_train, l_test = train_test_split(tmp_f, tmp_l, test_size=0.3, random_state=23)\n",
    "\n",
    "clf.fit(f_train, l_train.reshape(-1))\n",
    "prediction = clf.predict(f_test)\n",
    "\n",
    "print(mean_absolute_error(l_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression + Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "features_poly = poly.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split for train and test sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features_poly, labels, test_size=0.3, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0464094046792\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#clf = LinearRegression(fit_intercept=False, n_jobs=-1)\n",
    "clf = Lasso(alpha = 0.1)\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0327128646758\n",
      "{'max_depth': 6, 'min_samples_leaf': 7, 'min_samples_split': 2}\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = DecisionTreeRegressor(random_state=23)\n",
    "\n",
    "parameters_grid = {\n",
    "    \"max_depth\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4, 5, 6, 7],\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(clf, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "gcv.fit(features_train, labels_train.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023588186645\n",
      "{'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
      "Wall time: 11min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RFR = RandomForestRegressor(n_jobs=-1, random_state=23)\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": [200, 250, 300, 350, 400],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(RFR, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "gcv.fit(features, labels.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0223665344854\n"
     ]
    }
   ],
   "source": [
    "#clf = gcv.best_estimator_\n",
    "clf = RandomForestRegressor(n_estimators=350, min_samples_leaf=2, min_samples_split=2, n_jobs=-1, random_state=23)\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['RF'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0207490017856\n",
      "Wall time: 14min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=300, learning_rate=0.7, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results_ada['RF'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0315106462462\n",
      "{'n_neighbors': 4}\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_neighbors\": list(range(1, 15)),\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(clf, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "gcv.fit(features_train, labels_train.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = gcv.best_estimator_\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "results['KNN'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_KernelRidge(X, y):\n",
    "    clf = KernelRidge()\n",
    "\n",
    "    parameters_grid = {\n",
    "        \"kernel\": ['polynomial', 'rbf'], \n",
    "        \"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "        \"gamma\": np.logspace(-3, 3, 10)    \n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(clf, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0251215396392\n",
      "{'alpha': 0.1, 'gamma': 0.0046415888336127772, 'kernel': 'polynomial'}\n",
      "0.0230851795018\n"
     ]
    }
   ],
   "source": [
    "clf = choose_KernelRidge(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['KRG'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0262928641152\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=100, learning_rate=1.0, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results_ada['KRG'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_ada['KRG'] = results['KRG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0529154749451\n",
      "{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SVR()\n",
    "\n",
    "parameters_grid = {\n",
    "    \"kernel\": ['rbf', 'poly'],\n",
    "    \"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "    \"gamma\": np.logspace(-2, 2, 5)\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(clf, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "gcv.fit(features_train, labels_train.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = gcv.best_estimator_\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "results['SVR'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_XGB(X, y):\n",
    "    XGB = xgboost.XGBRegressor()\n",
    "\n",
    "    parameters_grid = {\n",
    "        #\"n_estimators\": [50, 100, 150, 200, 250, 300],\n",
    "        \"n_estimators\": [250, 300, 350],\n",
    "        #\"max_depth\": [2, 3, 4, 5, 6, 7, 8],\n",
    "        \"max_depth\": [3, 4, 5],\n",
    "        #\"learning_rate\": [0.1, 0.05, 0.01]\n",
    "        #\"learning_rate\": [0.1],\n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(XGB, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0227337351622\n",
      "{'max_depth': 4, 'n_estimators': 300}\n",
      "0.0204025974813\n",
      "Wall time: 53.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = choose_XGB(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "#results['XGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0200851372074\n"
     ]
    }
   ],
   "source": [
    "XGB = xgboost.XGBRegressor(max_depth=3, n_estimators=350)\n",
    "XGB.fit(features_train, labels_train.reshape(-1))\n",
    "\n",
    "prediction = XGB.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['XGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0194865040989\n",
      "Wall time: 12min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=XGB, n_estimators=300, learning_rate=0.5, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results_ada['XGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0217873663087\n",
      "{'learning_rate': 0.5, 'n_estimators': 300}\n",
      "Wall time: 4h 27min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XGB = xgboost.XGBRegressor(max_depth=3, n_estimators=350)\n",
    "adb_clf = AdaBoostRegressor(base_estimator=XGB, random_state=23)\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": [300, 350, 400],\n",
    "    \"learning_rate\": [0.5, 0.6, 0.7],\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(adb_clf, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "gcv.fit(features_train, labels_train.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018971133894\n",
      "(0.20000000000000001, 0.30000000000000004, 0.0, 0.0, 0.5)\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = []\n",
    "wgh = []\n",
    "\n",
    "for c1 in np.linspace(0.0, 0.9, 10):\n",
    "    for c2 in np.linspace(0.1, 0.6, 6):\n",
    "        if(c1 + c2 > 1.01):\n",
    "            break\n",
    "        else:\n",
    "            for c3 in np.linspace(0.0, 0.9, 10):\n",
    "                if(c1 + c2 + c3 > 1.01):\n",
    "                    break\n",
    "                else:\n",
    "                    for c4 in np.linspace(0.0, 0.9, 10):\n",
    "                        if(c1 + c2 + c3 + c4 > 1.01):\n",
    "                            break\n",
    "                        else:\n",
    "                            for c5 in np.linspace(0.0, 0.9, 10):\n",
    "                                if(c1 + c2 + c3 + c4 + c5 > 1.01):\n",
    "                                    break\n",
    "                                else:\n",
    "                                    tmp = c1*results_ada['KRG'] + c2*results_ada['RF'] + c3*results_ada['LGB'] + c4*results_ada['GBM'] + c5*results_ada['XGB']\n",
    "                                    res.append(mean_absolute_error(results.Target, tmp))\n",
    "                                    wgh.append( (c1, c2, c3, c4, c5) )\n",
    "                                    \n",
    "print(min(res))\n",
    "idx = argmin(res)\n",
    "print(wgh[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.0, 0.6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0190728184582\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(results_ada[['LGB', 'GBM', 'XGB', 'KRG', 'RF']], axis=1)\n",
    "\n",
    "print(\"Average vs Target\", mean_absolute_error(results_ada.Target, average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0193590982883\n",
      "GBM vs Target 0.0201937590676\n",
      "Manual_kNN vs Target 0.0287775614624\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(results[['GBM', 'RF', 'XGB', 'K_Ridge']], axis=1)\n",
    "\n",
    "print(\"Average vs Target\", mean_absolute_error(results.Target, average))\n",
    "print(\"GBM vs Target\", mean_absolute_error(results.Target, results.GBM))\n",
    "print(\"Manual_kNN vs Target\", mean_absolute_error(results.Target, results.Manual_kNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0197486344607\n"
     ]
    }
   ],
   "source": [
    "tmp = np.mean(results[['GBM', 'K_Ridge']], axis=1)\n",
    "print(\"Average vs Target\", mean_absolute_error(results.Target, tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0191947267387\n"
     ]
    }
   ],
   "source": [
    "#average = np.mean(results[['MLP', 'LGB', 'GBM', 'RF', 'XGB', 'K_Ridge']], axis=1)\n",
    "average = np.mean(results[['LGB', 'GBM', 'XGB', 'KRG', 'RF']], axis=1)\n",
    "\n",
    "print(\"Average vs Target\", mean_absolute_error(results.Target, average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0269612525144\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(results.loc[:, results.columns != 'Target'], axis=1)\n",
    "\n",
    "print(\"Average vs Target\", mean_absolute_error(results.Target, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-2c83b60bea02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstack_l_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_ada\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_f_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_l_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_f_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m   1216\u001b[0m                          order=\"C\")\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(n_jobs=-1, random_state=23)\n",
    "\n",
    "stack_f_train = results_ada.loc[:, results_ada.columns != 'Target'].as_matrix()\n",
    "stack_l_train = results_ada['Target'].as_matrix().reshape(-1, 1)\n",
    "\n",
    "LR.fit(stack_f_train, stack_l_train)\n",
    "prediction = LR.predict(stack_f_train)\n",
    "\n",
    "print(mean_absolute_error(results_ada['Target'], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    \"\"   \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253404</td>\n",
       "      <td>0.284106</td>\n",
       "      <td>0.226146</td>\n",
       "      <td>0.057960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336283</td>\n",
       "      <td>0.582781</td>\n",
       "      <td>0.486312</td>\n",
       "      <td>0.096469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303438</td>\n",
       "      <td>0.434437</td>\n",
       "      <td>0.356958</td>\n",
       "      <td>0.077479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294078</td>\n",
       "      <td>0.249007</td>\n",
       "      <td>0.192849</td>\n",
       "      <td>0.056157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260892</td>\n",
       "      <td>0.370861</td>\n",
       "      <td>0.286522</td>\n",
       "      <td>0.084339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249404</td>\n",
       "      <td>0.309934</td>\n",
       "      <td>0.246559</td>\n",
       "      <td>0.063375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285824</td>\n",
       "      <td>0.389404</td>\n",
       "      <td>0.330432</td>\n",
       "      <td>0.058972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233833</td>\n",
       "      <td>0.319868</td>\n",
       "      <td>0.266475</td>\n",
       "      <td>0.053392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.291014</td>\n",
       "      <td>0.493248</td>\n",
       "      <td>0.409090</td>\n",
       "      <td>0.084158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257573</td>\n",
       "      <td>0.339073</td>\n",
       "      <td>0.276054</td>\n",
       "      <td>0.063019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362151</td>\n",
       "      <td>0.533775</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.066270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336964</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.431042</td>\n",
       "      <td>0.077527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332539</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.363911</td>\n",
       "      <td>0.159268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243022</td>\n",
       "      <td>0.344371</td>\n",
       "      <td>0.276917</td>\n",
       "      <td>0.067454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242852</td>\n",
       "      <td>0.311258</td>\n",
       "      <td>0.243552</td>\n",
       "      <td>0.067706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295014</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.345745</td>\n",
       "      <td>0.054255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.355003</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>0.386137</td>\n",
       "      <td>0.110552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273911</td>\n",
       "      <td>0.366887</td>\n",
       "      <td>0.315304</td>\n",
       "      <td>0.051584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.810101</td>\n",
       "      <td>0.189899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.276719</td>\n",
       "      <td>0.519868</td>\n",
       "      <td>0.369746</td>\n",
       "      <td>0.150122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...     618  619  \\\n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0    ...     0.0  0.0   \n",
       "9    1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "23   0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "36   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "49   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "129  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "141  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "160  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "241  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "245  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "282  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "305  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "323  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "332  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "394  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "398  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "401  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "407  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "423  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "426  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     0.0  0.0   \n",
       "\n",
       "     620  621  622  623       624  SalePrice  Prediction       Dif  \n",
       "1    0.0  0.0  1.0  0.0  0.253404   0.284106    0.226146  0.057960  \n",
       "9    0.0  0.0  0.0  1.0  0.336283   0.582781    0.486312  0.096469  \n",
       "23   0.0  0.0  1.0  0.0  0.303438   0.434437    0.356958  0.077479  \n",
       "36   0.0  1.0  0.0  0.0  0.294078   0.249007    0.192849  0.056157  \n",
       "49   0.0  0.0  1.0  0.0  0.260892   0.370861    0.286522  0.084339  \n",
       "129  0.0  0.0  1.0  0.0  0.249404   0.309934    0.246559  0.063375  \n",
       "141  0.0  0.0  1.0  0.0  0.285824   0.389404    0.330432  0.058972  \n",
       "160  0.0  0.0  1.0  0.0  0.233833   0.319868    0.266475  0.053392  \n",
       "241  0.0  0.0  0.0  1.0  0.291014   0.493248    0.409090  0.084158  \n",
       "245  0.0  0.0  1.0  0.0  0.257573   0.339073    0.276054  0.063019  \n",
       "282  0.0  0.0  1.0  0.0  0.362151   0.533775    0.467505  0.066270  \n",
       "305  0.0  0.0  0.0  1.0  0.336964   0.508570    0.431042  0.077527  \n",
       "323  0.0  0.0  0.0  1.0  0.332539   0.523179    0.363911  0.159268  \n",
       "332  0.0  0.0  1.0  0.0  0.243022   0.344371    0.276917  0.067454  \n",
       "394  0.0  0.0  1.0  0.0  0.242852   0.311258    0.243552  0.067706  \n",
       "398  0.0  0.0  1.0  0.0  0.295014   0.400000    0.345745  0.054255  \n",
       "401  0.0  0.0  1.0  0.0  0.355003   0.496689    0.386137  0.110552  \n",
       "407  0.0  0.0  1.0  0.0  0.273911   0.366887    0.315304  0.051584  \n",
       "423  0.0  0.0  1.0  0.0  0.575221   1.000000    0.810101  0.189899  \n",
       "426  0.0  0.0  0.0  1.0  0.276719   0.519868    0.369746  0.150122  \n",
       "\n",
       "[20 rows x 628 columns]"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1 = pd.DataFrame(features_test)\n",
    "tmp1[\"SalePrice\"] = labels_test\n",
    "tmp1[\"Prediction\"] = prediction\n",
    "tmp1[\"Dif\"] = tmp1[\"SalePrice\"] - tmp1[\"Prediction\"]\n",
    "tmp1[tmp1[\"Dif\"] > 0.05]\n",
    "#tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_guy = features_test[323, :]\n",
    "\n",
    "dct = {}\n",
    "\n",
    "for ii in range(features_train.shape[0]):\n",
    "    dct[ii] = np.linalg.norm(features_train[ii, :] - bad_guy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(679, 4.0572058981167798),\n",
       " (460, 4.1037883500069876),\n",
       " (48, 4.2623789384980393),\n",
       " (397, 4.274175333909576),\n",
       " (190, 4.3767181932109667),\n",
       " (57, 4.4168297180683345),\n",
       " (64, 4.4428056354746825),\n",
       " (226, 4.5846531235730303),\n",
       " (157, 4.596488056804513),\n",
       " (930, 4.6632828726130615),\n",
       " (47, 4.6999548322582809),\n",
       " (283, 4.7014405236404277),\n",
       " (75, 4.7381908611740302),\n",
       " (634, 4.7580281455803854),\n",
       " (68, 4.7615236139951094),\n",
       " (757, 4.784261782273119),\n",
       " (15, 4.8071201074491876),\n",
       " (140, 4.8135197646579488),\n",
       " (974, 4.8370012529865019),\n",
       " (997, 4.8398494076218439),\n",
       " (212, 4.8497802279082824),\n",
       " (625, 4.8538997766605068),\n",
       " (994, 4.8723637666820618),\n",
       " (311, 4.8766821801573776),\n",
       " (672, 4.9114913636506818),\n",
       " (633, 4.9362610722979419),\n",
       " (767, 4.9406670382759019),\n",
       " (1004, 4.9642315957389869),\n",
       " (90, 4.9657207627467672),\n",
       " (245, 4.9857175656754915),\n",
       " (424, 5.0168802429649624),\n",
       " (861, 5.017664613541565),\n",
       " (870, 5.0507985691112127),\n",
       " (851, 5.1177925848144046),\n",
       " (155, 5.1377546699455685),\n",
       " (539, 5.1561584209492919),\n",
       " (159, 5.1620624923853473),\n",
       " (665, 5.1665438063283595),\n",
       " (451, 5.1739333117217061),\n",
       " (257, 5.1770141029227359),\n",
       " (884, 5.2026152801688506),\n",
       " (923, 5.2043405611752478),\n",
       " (947, 5.2199152798984292),\n",
       " (454, 5.2227612601520788),\n",
       " (662, 5.2301489209678822),\n",
       " (531, 5.2351346431838843),\n",
       " (0, 5.254496022361745),\n",
       " (989, 5.25987659932788),\n",
       " (375, 5.3045332560414948),\n",
       " (546, 5.311110515767024),\n",
       " (881, 5.3119306974634206),\n",
       " (116, 5.3155769305823082),\n",
       " (808, 5.3226504332211766),\n",
       " (980, 5.3279125764536195),\n",
       " (302, 5.3309002463922051),\n",
       " (831, 5.3333268008148744),\n",
       " (444, 5.3359731027209039),\n",
       " (555, 5.3388481900518672),\n",
       " (186, 5.3479500975686411),\n",
       " (1021, 5.3497317313251056),\n",
       " (348, 5.3514941792826303),\n",
       " (325, 5.354038452102996),\n",
       " (336, 5.364869359842575),\n",
       " (897, 5.3666615188769518),\n",
       " (124, 5.3793992758314788),\n",
       " (701, 5.3957429852990737),\n",
       " (844, 5.4030086454965298),\n",
       " (343, 5.4210393671801587),\n",
       " (748, 5.4210851705838943),\n",
       " (848, 5.4242687282780233),\n",
       " (154, 5.4299384980410119),\n",
       " (461, 5.4366114213805021),\n",
       " (92, 5.4393830028169905),\n",
       " (312, 5.4469201565134888),\n",
       " (578, 5.4583161061072767),\n",
       " (506, 5.468150817926916),\n",
       " (101, 5.4883451131921479),\n",
       " (247, 5.4951243386401698),\n",
       " (626, 5.511790373097301),\n",
       " (115, 5.5240575777598773),\n",
       " (479, 5.5285848217863771),\n",
       " (724, 5.5286913208683712),\n",
       " (387, 5.5297292345266058),\n",
       " (807, 5.5324288964222701),\n",
       " (437, 5.5401075981950916),\n",
       " (137, 5.5456734645749863),\n",
       " (443, 5.5629698176485034),\n",
       " (58, 5.5641954798832414),\n",
       " (306, 5.5687336218385992),\n",
       " (962, 5.5960599748873401),\n",
       " (401, 5.5981318220831522),\n",
       " (361, 5.6058395056321739),\n",
       " (696, 5.6177843308655433),\n",
       " (876, 5.6194830533387821),\n",
       " (584, 5.6204360068450629),\n",
       " (845, 5.6239197152072906),\n",
       " (630, 5.6242177467261518),\n",
       " (95, 5.6478302594485505),\n",
       " (899, 5.6527521415959523),\n",
       " (749, 5.6530217306585815),\n",
       " (33, 5.6621403649037827),\n",
       " (714, 5.6722482632819569),\n",
       " (99, 5.6824741987698975),\n",
       " (948, 5.6843694815989503),\n",
       " (193, 5.695641771019778),\n",
       " (927, 5.6986065137139335),\n",
       " (810, 5.7029335013379763),\n",
       " (9, 5.7069612130661147),\n",
       " (168, 5.7095023498617072),\n",
       " (917, 5.7115099445853055),\n",
       " (12, 5.7142462901526621),\n",
       " (113, 5.7146211822805553),\n",
       " (622, 5.7182767245707389),\n",
       " (187, 5.7205277038391884),\n",
       " (418, 5.7208222760936902),\n",
       " (667, 5.7236122905998243),\n",
       " (939, 5.7237329060390696),\n",
       " (71, 5.7248963381781),\n",
       " (801, 5.7266382735196197),\n",
       " (327, 5.72967676783108),\n",
       " (204, 5.7527666643894655),\n",
       " (1013, 5.7668639211061379),\n",
       " (731, 5.7698682539370747),\n",
       " (882, 5.7786692655480483),\n",
       " (55, 5.7823835609614962),\n",
       " (196, 5.7850625287585276),\n",
       " (570, 5.7866271471169792),\n",
       " (309, 5.7909469169821719),\n",
       " (359, 5.794483349686363),\n",
       " (769, 5.8011570595781947),\n",
       " (499, 5.8090921729081364),\n",
       " (624, 5.8121560572717934),\n",
       " (381, 5.8286688935636581),\n",
       " (73, 5.8295898315874943),\n",
       " (1, 5.8399011709902888),\n",
       " (779, 5.8445871963031006),\n",
       " (334, 5.8463183470984639),\n",
       " (65, 5.8553565543109674),\n",
       " (285, 5.8610383535935497),\n",
       " (653, 5.8618764448876393),\n",
       " (833, 5.8783695570997603),\n",
       " (834, 5.8907111144093314),\n",
       " (229, 5.8949895605431317),\n",
       " (323, 5.8956150814092076),\n",
       " (765, 5.8973071190043358),\n",
       " (888, 5.8991354041983675),\n",
       " (436, 5.9048103609473017),\n",
       " (528, 5.908798380413093),\n",
       " (429, 5.9244706276444976),\n",
       " (795, 5.9262584465633346),\n",
       " (112, 5.9262836981275253),\n",
       " (450, 5.9280951346111053),\n",
       " (175, 5.9303468875254604),\n",
       " (514, 5.9328438780270325),\n",
       " (920, 5.9349756674683505),\n",
       " (162, 5.9382435516379717),\n",
       " (318, 5.9388172716552141),\n",
       " (432, 5.9397248774336626),\n",
       " (516, 5.9404141648110542),\n",
       " (449, 5.940636521005918),\n",
       " (647, 5.9436604881071071),\n",
       " (169, 5.945447350206706),\n",
       " (468, 5.9494455849579619),\n",
       " (709, 5.9541750135123745),\n",
       " (329, 5.9634170382594602),\n",
       " (402, 5.9654990551098139),\n",
       " (983, 5.9692250264558364),\n",
       " (487, 5.9706603211436811),\n",
       " (383, 5.9792868686600107),\n",
       " (842, 5.9837322665370145),\n",
       " (40, 6.003886924119068),\n",
       " (787, 6.0060544509951175),\n",
       " (710, 6.0069383900968667),\n",
       " (445, 6.0404496285222642),\n",
       " (603, 6.0450748210151497),\n",
       " (50, 6.048756961912563),\n",
       " (644, 6.0494886397793728),\n",
       " (261, 6.0505076225440151),\n",
       " (227, 6.0513297639223982),\n",
       " (122, 6.0625724692768097),\n",
       " (868, 6.0653798853955383),\n",
       " (802, 6.0739585489714392),\n",
       " (556, 6.0849984297066877),\n",
       " (689, 6.0861558219875844),\n",
       " (598, 6.086736992815907),\n",
       " (887, 6.0897944004018081),\n",
       " (141, 6.0934191711257384),\n",
       " (2, 6.0975465766220864),\n",
       " (1020, 6.1040979823258148),\n",
       " (572, 6.104643283452158),\n",
       " (945, 6.1147586815421455),\n",
       " (153, 6.1316806276324138),\n",
       " (287, 6.1340243735440074),\n",
       " (605, 6.1342448286723181),\n",
       " (663, 6.1386971239634942),\n",
       " (477, 6.1392853335614106),\n",
       " (207, 6.1436786152256966),\n",
       " (687, 6.1553998501656713),\n",
       " (547, 6.1587461912872055),\n",
       " (636, 6.1653984367224686),\n",
       " (895, 6.1772831242093611),\n",
       " (740, 6.1815656682421549),\n",
       " (111, 6.1849274345234582),\n",
       " (755, 6.1881055771976801),\n",
       " (965, 6.1953709509549064),\n",
       " (455, 6.2020940906888589),\n",
       " (717, 6.2073092361883919),\n",
       " (813, 6.2129924773092009),\n",
       " (494, 6.2140766042823534),\n",
       " (132, 6.2239497126826437),\n",
       " (298, 6.2314064500883628),\n",
       " (942, 6.2393649262743045),\n",
       " (746, 6.2409071800194287),\n",
       " (249, 6.2453320383481925),\n",
       " (89, 6.2468329807755589),\n",
       " (417, 6.2507362332337602),\n",
       " (794, 6.2657108304453297),\n",
       " (214, 6.2682987114855946),\n",
       " (107, 6.2847320755591607),\n",
       " (615, 6.2904633368792089),\n",
       " (814, 6.2920357990969213),\n",
       " (875, 6.2950624975881073),\n",
       " (535, 6.295353963417055),\n",
       " (906, 6.2966220229937173),\n",
       " (751, 6.3008068331602418),\n",
       " (379, 6.3131780145009069),\n",
       " (54, 6.3158635322245766),\n",
       " (680, 6.3196081031390055),\n",
       " (790, 6.3214505829011385),\n",
       " (51, 6.3233611055111352),\n",
       " (17, 6.3353828094385181),\n",
       " (824, 6.3374046441395757),\n",
       " (330, 6.338615444434696),\n",
       " (950, 6.3395738729203774),\n",
       " (936, 6.3555807417864241),\n",
       " (872, 6.3561675474133041),\n",
       " (289, 6.3566896223185179),\n",
       " (475, 6.356707664465385),\n",
       " (937, 6.3631755752752275),\n",
       " (904, 6.365049294669304),\n",
       " (253, 6.3662757902881353),\n",
       " (700, 6.3744033258898547),\n",
       " (337, 6.374686051876556),\n",
       " (892, 6.3872621077950695),\n",
       " (949, 6.3914212740771417),\n",
       " (25, 6.3949874523359123),\n",
       " (79, 6.3989918706939326),\n",
       " (736, 6.4060336784328413),\n",
       " (763, 6.411436483583052),\n",
       " (370, 6.4133885498780705),\n",
       " (562, 6.4148251849755376),\n",
       " (811, 6.4183162277062253),\n",
       " (104, 6.4246848204718319),\n",
       " (23, 6.4269282824018186),\n",
       " (629, 6.4290264131197681),\n",
       " (632, 6.4324047951990568),\n",
       " (1008, 6.4341000326593454),\n",
       " (103, 6.4341275785242171),\n",
       " (446, 6.4551920038581887),\n",
       " (130, 6.4630375622560479),\n",
       " (275, 6.4636080739868627),\n",
       " (559, 6.4741643676928735),\n",
       " (300, 6.4788278387263896),\n",
       " (675, 6.4842555684217205),\n",
       " (595, 6.4856707929651698),\n",
       " (315, 6.493743413424613),\n",
       " (407, 6.5062948537291874),\n",
       " (180, 6.5132484922165643),\n",
       " (466, 6.5368618344877891),\n",
       " (209, 6.543574096332665),\n",
       " (809, 6.5477429459585261),\n",
       " (413, 6.550714710906008),\n",
       " (758, 6.5631806978853637),\n",
       " (484, 6.5649396371028343),\n",
       " (866, 6.5654199124884771),\n",
       " (782, 6.5667408997795347),\n",
       " (580, 6.5709320824256139),\n",
       " (959, 6.5789148092230185),\n",
       " (695, 6.5821250129677269),\n",
       " (390, 6.5824168273655559),\n",
       " (885, 6.5889528599367893),\n",
       " (837, 6.5978918893148197),\n",
       " (566, 6.6086695800059188),\n",
       " (518, 6.6103288044389421),\n",
       " (733, 6.6154753410674934),\n",
       " (386, 6.6188778909207606),\n",
       " (319, 6.6218685426140347),\n",
       " (314, 6.6229139334119385),\n",
       " (259, 6.6294480958914619),\n",
       " (734, 6.634553930680025),\n",
       " (654, 6.6349928792101265),\n",
       " (682, 6.6367422906923785),\n",
       " (384, 6.6400416487282028),\n",
       " (105, 6.6414216866155869),\n",
       " (713, 6.645332077693558),\n",
       " (425, 6.6517123324407841),\n",
       " (551, 6.659455251689578),\n",
       " (5, 6.6649685591177521),\n",
       " (49, 6.6656597814184506),\n",
       " (805, 6.6874112308861688),\n",
       " (166, 6.6878640933139923),\n",
       " (803, 6.6879135649982713),\n",
       " (483, 6.687930512110916),\n",
       " (697, 6.687930512110916),\n",
       " (22, 6.6958793478799912),\n",
       " (87, 6.7025802601991984),\n",
       " (331, 6.7044101090073882),\n",
       " (398, 6.7069754505500949),\n",
       " (976, 6.7096488644745014),\n",
       " (184, 6.7322775617915349),\n",
       " (415, 6.7343861252479744),\n",
       " (987, 6.7387322447775917),\n",
       " (822, 6.753562347649936),\n",
       " (286, 6.7596922211816706),\n",
       " (527, 6.760742027746157),\n",
       " (784, 6.7634767490675776),\n",
       " (561, 6.7646755087178683),\n",
       " (645, 6.770465304434067),\n",
       " (628, 6.7726601695923963),\n",
       " (239, 6.774874108242301),\n",
       " (273, 6.7766262518779019),\n",
       " (913, 6.779235330099505),\n",
       " (839, 6.7842415690589606),\n",
       " (964, 6.7854752659298256),\n",
       " (416, 6.8101300116627872),\n",
       " (37, 6.8106207800762819),\n",
       " (858, 6.8110210025945719),\n",
       " (78, 6.8182062807355566),\n",
       " (928, 6.8413959238197783),\n",
       " (953, 6.8424654119656321),\n",
       " (984, 6.844286877380731),\n",
       " (771, 6.8621411521679327),\n",
       " (74, 6.8667273125084005),\n",
       " (586, 6.873707578716413),\n",
       " (720, 6.877872169476678),\n",
       " (541, 6.895179045365408),\n",
       " (806, 6.9034132360679843),\n",
       " (614, 6.9078257262702474),\n",
       " (914, 6.9085381605047917),\n",
       " (44, 6.912281236399652),\n",
       " (45, 6.9144603383531598),\n",
       " (905, 6.9163231800583658),\n",
       " (825, 6.9170913907705147),\n",
       " (172, 6.922300037789185),\n",
       " (864, 6.9243309890526872),\n",
       " (816, 6.9268120257508965),\n",
       " (862, 6.9409552334368936),\n",
       " (823, 6.9426488672900897),\n",
       " (788, 6.9432073355044412),\n",
       " (855, 6.9454052126595434),\n",
       " (981, 6.9486576464015446),\n",
       " (922, 6.9518637119586764),\n",
       " (812, 6.9562745799521206),\n",
       " (66, 6.9629513706980255),\n",
       " (496, 6.9677099662673099),\n",
       " (846, 6.977848433710383),\n",
       " (780, 6.9817083992304187),\n",
       " (34, 6.9847055058346017),\n",
       " (952, 6.9879291200780118),\n",
       " (871, 6.9921081766918594),\n",
       " (711, 6.9954496833483724),\n",
       " (727, 6.9989767292040703),\n",
       " (233, 6.9999952479477399),\n",
       " (84, 7.0041556452959801),\n",
       " (164, 7.0064013863754457),\n",
       " (752, 7.0125923010506916),\n",
       " (718, 7.0187178292586907),\n",
       " (508, 7.0252564278309588),\n",
       " (761, 7.0302110039155492),\n",
       " (986, 7.0329234053193437),\n",
       " (910, 7.0345632786204213),\n",
       " (328, 7.0348474689883229),\n",
       " (391, 7.0399707458196525),\n",
       " (262, 7.045359658549061),\n",
       " (704, 7.0539380404022669),\n",
       " (543, 7.064928452637977),\n",
       " (631, 7.0650926399605112),\n",
       " (366, 7.0679961855392435),\n",
       " (462, 7.0778419380236253),\n",
       " (509, 7.0828929905437796),\n",
       " (292, 7.0909350747406172),\n",
       " (297, 7.0922120050589221),\n",
       " (422, 7.1010832154063532),\n",
       " (735, 7.1052027142038936),\n",
       " (46, 7.1104136507491944),\n",
       " (960, 7.1281999162146041),\n",
       " (216, 7.1304334006664325),\n",
       " (515, 7.152130970122796),\n",
       " (171, 7.1595211666109497),\n",
       " (121, 7.1608048785712652),\n",
       " (69, 7.1666164806420598),\n",
       " (754, 7.1679564985255233),\n",
       " (938, 7.1681262072089771),\n",
       " (608, 7.1682477545548275),\n",
       " (360, 7.1727598003707262),\n",
       " (463, 7.1789448811393761),\n",
       " (31, 7.1834643235078106),\n",
       " (498, 7.1854837394068216),\n",
       " (344, 7.1929245736983125),\n",
       " (235, 7.1953728859367549),\n",
       " (726, 7.1979569418776883),\n",
       " (639, 7.2020366973437193),\n",
       " (471, 7.2087318315497368),\n",
       " (482, 7.208957501386104),\n",
       " (1016, 7.2124406919763384),\n",
       " (339, 7.2144932164778046),\n",
       " (183, 7.2172986196782585),\n",
       " (255, 7.2185059158032354),\n",
       " (267, 7.2385348699347896),\n",
       " (890, 7.2468093080535487),\n",
       " (943, 7.2487028184327524),\n",
       " (610, 7.2618682605833893),\n",
       " (8, 7.2702755932464029),\n",
       " (907, 7.2751939613597685),\n",
       " (955, 7.2958947439176907),\n",
       " (393, 7.3030808670975942),\n",
       " (265, 7.3047247482745297),\n",
       " (768, 7.3113395517425133),\n",
       " (191, 7.3199172175964486),\n",
       " (29, 7.3240115469507723),\n",
       " (469, 7.3249568994718812),\n",
       " (673, 7.3272169877847775),\n",
       " (38, 7.3298298374501316),\n",
       " (317, 7.3318549237129123),\n",
       " (258, 7.3319890043573732),\n",
       " (838, 7.3354713321105827),\n",
       " (1015, 7.3385718317096522),\n",
       " (969, 7.3421436568446623),\n",
       " (392, 7.3458356043468127),\n",
       " (954, 7.3487337749746118),\n",
       " (820, 7.3597894322505688),\n",
       " (371, 7.361819887061956),\n",
       " (575, 7.3620111331569875),\n",
       " (238, 7.3621758936942134),\n",
       " (649, 7.370137980391311),\n",
       " (156, 7.3703529477934806),\n",
       " (935, 7.3715782962959135),\n",
       " (299, 7.37962540629304),\n",
       " (353, 7.3804172567962283),\n",
       " (431, 7.3929596544511362),\n",
       " (789, 7.3942295314491364),\n",
       " (345, 7.3960983268967588),\n",
       " (521, 7.3961778392757074),\n",
       " (777, 7.3968192373143582),\n",
       " (251, 7.4116417038163478),\n",
       " (222, 7.4135678854242748),\n",
       " (52, 7.4151987237826864),\n",
       " (638, 7.4167538993845872),\n",
       " (182, 7.4200533419578774),\n",
       " (503, 7.4207225909842096),\n",
       " (867, 7.4222613292042627),\n",
       " (96, 7.4236611876217511),\n",
       " (640, 7.4276969580319063),\n",
       " (464, 7.4296136166682585),\n",
       " (152, 7.4298199918331509),\n",
       " (338, 7.4308327971034558),\n",
       " (719, 7.4308364677511181),\n",
       " (873, 7.4384704456830191),\n",
       " (304, 7.4386921264962718),\n",
       " (282, 7.4405142981520127),\n",
       " (973, 7.4502992212591073),\n",
       " (594, 7.4521686463030274),\n",
       " (433, 7.4585680289144625),\n",
       " (924, 7.4648894129864454),\n",
       " (243, 7.4683698077046383),\n",
       " (188, 7.4759679440860847),\n",
       " (139, 7.4889040995364402),\n",
       " (849, 7.4890600262841165),\n",
       " (82, 7.4916015900952919),\n",
       " (843, 7.4961921215286882),\n",
       " (100, 7.4968741759877062),\n",
       " (728, 7.5004220614822152),\n",
       " (322, 7.5008739906170927),\n",
       " (480, 7.5015430163532315),\n",
       " (684, 7.5033952715184169),\n",
       " (738, 7.507024653198509),\n",
       " (310, 7.5093418907439089),\n",
       " (367, 7.5110458057316158),\n",
       " (434, 7.5200565458060877),\n",
       " (147, 7.5249227943365611),\n",
       " (540, 7.5294289665687204),\n",
       " (577, 7.5306258826093035),\n",
       " (199, 7.5315485763094872),\n",
       " (511, 7.5351491014455174),\n",
       " (850, 7.5353263836279414),\n",
       " (916, 7.5361241337111506),\n",
       " (963, 7.5401454918219555),\n",
       " (988, 7.5434310626387715),\n",
       " (505, 7.5454221555816705),\n",
       " (542, 7.5474555994669181),\n",
       " (308, 7.5510526370415842),\n",
       " (815, 7.5517199288919397),\n",
       " (77, 7.5519008424560834),\n",
       " (827, 7.5617441849429712),\n",
       " (81, 7.5636060068868458),\n",
       " (703, 7.573526050121191),\n",
       " (351, 7.5759585183947049),\n",
       " (491, 7.5765278702194907),\n",
       " (86, 7.5810798561353234),\n",
       " (968, 7.5833716200227475),\n",
       " (221, 7.5834444731068693),\n",
       " (61, 7.5868922713552367),\n",
       " (676, 7.5891179391096788),\n",
       " (405, 7.5930655056361482),\n",
       " (377, 7.5949338200080643),\n",
       " (712, 7.5951175939836304),\n",
       " (517, 7.5996420254976957),\n",
       " (804, 7.6092209912940216),\n",
       " (854, 7.6271204283007226),\n",
       " (30, 7.628659144453299),\n",
       " (744, 7.6293960140827624),\n",
       " (791, 7.6392160963070737),\n",
       " (683, 7.6411733919549816),\n",
       " (729, 7.6441783110429675),\n",
       " (271, 7.6450925878530533),\n",
       " (512, 7.6503244116917219),\n",
       " (341, 7.6655444775521664),\n",
       " (268, 7.6673775697010003),\n",
       " (934, 7.6701080228534018),\n",
       " (742, 7.6709284153359443),\n",
       " (138, 7.6744974315434638),\n",
       " (778, 7.6776701857891618),\n",
       " (352, 7.680272738471162),\n",
       " (232, 7.6803822737390295),\n",
       " (165, 7.6804060268388561),\n",
       " (388, 7.6822478185359229),\n",
       " (785, 7.689234430606934),\n",
       " (195, 7.6907820810875762),\n",
       " (510, 7.6915237193132739),\n",
       " (438, 7.6928812129399082),\n",
       " (478, 7.6956323855765492),\n",
       " (56, 7.6962194821810757),\n",
       " (886, 7.69909881348498),\n",
       " (688, 7.7010946576010397),\n",
       " (382, 7.7022449799030577),\n",
       " (201, 7.7043745587141759),\n",
       " (67, 7.705327237109735),\n",
       " (177, 7.7108920405785328),\n",
       " (617, 7.7141534215047569),\n",
       " (76, 7.71705489820756),\n",
       " (670, 7.7187658746937835),\n",
       " (655, 7.7190373592691497),\n",
       " (912, 7.7329224393727234),\n",
       " (799, 7.7419868480657748),\n",
       " (560, 7.7422355366970343),\n",
       " (293, 7.7493627142350334),\n",
       " (389, 7.7510046498377871),\n",
       " (378, 7.752047883887462),\n",
       " (847, 7.7597299504820665),\n",
       " (403, 7.777061448260115),\n",
       " (764, 7.7770935915904964),\n",
       " (999, 7.7828333749632135),\n",
       " (725, 7.7897365939704208),\n",
       " (320, 7.7928649372301226),\n",
       " (72, 7.801532767618113),\n",
       " (705, 7.8054838556754174),\n",
       " (42, 7.8066398527975025),\n",
       " (409, 7.8425050734554471),\n",
       " (495, 7.8449080465178405),\n",
       " (380, 7.8460254778514784),\n",
       " (933, 7.8480910619488427),\n",
       " (568, 7.8527427342765259),\n",
       " (291, 7.8542438027441568),\n",
       " (926, 7.8605604274397747),\n",
       " (619, 7.8677055587680362),\n",
       " (896, 7.8725887743270482),\n",
       " (538, 7.8734435792092441),\n",
       " (774, 7.8754189981826492),\n",
       " (604, 7.8759253108520042),\n",
       " (203, 7.8785900050072284),\n",
       " (410, 7.8798924879201007),\n",
       " (355, 7.8825377363087981),\n",
       " (828, 7.8838762500822686),\n",
       " (573, 7.8903574325928219),\n",
       " (16, 7.893656420926054),\n",
       " (358, 7.8976130789750796),\n",
       " (698, 7.8992613296577359),\n",
       " (975, 7.8999218633673003),\n",
       " (420, 7.9009780882821481),\n",
       " (28, 7.9011740576043339),\n",
       " (522, 7.9013326901714214),\n",
       " (470, 7.9026809848090167),\n",
       " (35, 7.9029896463078968),\n",
       " (10, 7.9036734730951332),\n",
       " (27, 7.9061361908784411),\n",
       " (350, 7.9101792318133102),\n",
       " (889, 7.9146160188315839),\n",
       " (7, 7.9185250090094712),\n",
       " (1017, 7.9245972435605481),\n",
       " (591, 7.9254239837757652),\n",
       " (170, 7.9275896712398275),\n",
       " (11, 7.9296173549722457),\n",
       " (956, 7.9315782290066048),\n",
       " (118, 7.9351190915315524),\n",
       " (970, 7.9357334406149391),\n",
       " (835, 7.9371449684242394),\n",
       " (324, 7.9391341110832636),\n",
       " (136, 7.9391930810885833),\n",
       " (829, 7.9393010812319549),\n",
       " (592, 7.9450015279737256),\n",
       " (277, 7.9469699280607564),\n",
       " (102, 7.9491486242894203),\n",
       " (883, 7.9502106217792932),\n",
       " (472, 7.9536274908350606),\n",
       " (641, 7.9543695886997288),\n",
       " (957, 7.9550063497208177),\n",
       " (234, 7.960695870234586),\n",
       " (4, 7.9650059586421422),\n",
       " (296, 7.9762977479534678),\n",
       " (264, 7.9802758367392785),\n",
       " (1019, 7.9822057075680162),\n",
       " (600, 7.9868616574126126),\n",
       " (781, 7.9903985715263959),\n",
       " (146, 7.9940135868154645),\n",
       " (374, 8.0012500372927153),\n",
       " (254, 8.0040718341496735),\n",
       " (894, 8.0076749707229986),\n",
       " (929, 8.0078199182233334),\n",
       " (971, 8.0109337551286828),\n",
       " (280, 8.0149452222479791),\n",
       " (6, 8.0152721904804132),\n",
       " (294, 8.0162248073337583),\n",
       " (743, 8.0179113715393893),\n",
       " (732, 8.0179257475496843),\n",
       " (316, 8.0190298139592553),\n",
       " (340, 8.0226787202651053),\n",
       " (588, 8.0256709868389482),\n",
       " (585, 8.0334388225491047),\n",
       " (242, 8.0373235988900351),\n",
       " (571, 8.0432476695288369),\n",
       " (326, 8.0481575086426655),\n",
       " (741, 8.0533721640544442),\n",
       " (978, 8.0541064092163133),\n",
       " (524, 8.0588644803627378),\n",
       " (406, 8.0618130075912529),\n",
       " (194, 8.0653812638505205),\n",
       " (776, 8.0663557426895007),\n",
       " (276, 8.0663833663883846),\n",
       " (874, 8.067610359237495),\n",
       " (525, 8.0691653960440917),\n",
       " (365, 8.0730934517676101),\n",
       " (290, 8.0744091973750205),\n",
       " (623, 8.0752218033005523),\n",
       " (661, 8.0758840372058351),\n",
       " (423, 8.0767364668506918),\n",
       " (88, 8.0779740348511488),\n",
       " (185, 8.0822752606011932),\n",
       " (674, 8.0835156222249029),\n",
       " (513, 8.0925588136887932),\n",
       " (128, 8.0976928715124608),\n",
       " (408, 8.1000178920465409),\n",
       " (456, 8.1042201196575991),\n",
       " (821, 8.1070183535418696),\n",
       " (721, 8.1124888889445028),\n",
       " (167, 8.116792114342763),\n",
       " (123, 8.1186028156329115),\n",
       " (305, 8.1213032955386293),\n",
       " (548, 8.1233366087112344),\n",
       " (19, 8.1238556155931576),\n",
       " (770, 8.1298586112585056),\n",
       " (613, 8.1323253086071112),\n",
       " (759, 8.1379167795807028),\n",
       " (93, 8.1386337164495721),\n",
       " (819, 8.1392496252469773),\n",
       " (260, 8.1428610349072397),\n",
       " (217, 8.1430221056138343),\n",
       " (109, 8.1505244032992934),\n",
       " (97, 8.1579238806311913),\n",
       " (176, 8.158893761563716),\n",
       " (995, 8.1600695265022001),\n",
       " (616, 8.1613587509675298),\n",
       " (1011, 8.1619920375061419),\n",
       " (646, 8.1630336616334098),\n",
       " (685, 8.1660538896464647),\n",
       " (85, 8.1741929814664349),\n",
       " (747, 8.1753426006953482),\n",
       " (281, 8.1782138264720619),\n",
       " (961, 8.1808442473067906),\n",
       " (990, 8.1819873858619445),\n",
       " (488, 8.1906676512329035),\n",
       " (364, 8.1938598229208726),\n",
       " (24, 8.1957556512091561),\n",
       " (582, 8.1969951266059589),\n",
       " (131, 8.1996779798892057),\n",
       " (219, 8.2022360553228886),\n",
       " (476, 8.2045320108329278),\n",
       " (426, 8.2052069821513758),\n",
       " (627, 8.2063101349000132),\n",
       " (497, 8.2074677660069533),\n",
       " (567, 8.2084756987317213),\n",
       " (394, 8.2093088664139131),\n",
       " (762, 8.2122743829511169),\n",
       " (127, 8.2127673730515127),\n",
       " (507, 8.2133397484916433),\n",
       " (722, 8.2142032881157565),\n",
       " (699, 8.2164002825459796),\n",
       " (860, 8.2171577754380376),\n",
       " (284, 8.2201920407957161),\n",
       " (62, 8.2234579363386349),\n",
       " (412, 8.2301163784145306),\n",
       " (898, 8.2301498445378005),\n",
       " (606, 8.2312709843971721),\n",
       " (404, 8.2336793398350441),\n",
       " (558, 8.2353478206534092),\n",
       " (597, 8.2425544484239435),\n",
       " (841, 8.2441990253731596),\n",
       " (581, 8.2472538170724405),\n",
       " (400, 8.2538059367317231),\n",
       " (248, 8.2560294326041443),\n",
       " (982, 8.2608062677194702),\n",
       " (368, 8.2667905204711438),\n",
       " (792, 8.2711863218787514),\n",
       " (473, 8.2724243552542056),\n",
       " (565, 8.2768616664101184),\n",
       " (840, 8.2770311685265234),\n",
       " (439, 8.2803101373799795),\n",
       " (467, 8.2870036325639731),\n",
       " (593, 8.2892644489450813),\n",
       " (1001, 8.2939446535218408),\n",
       " (465, 8.2954503732084515),\n",
       " (564, 8.2962746190418066),\n",
       " (915, 8.298387199475588),\n",
       " (760, 8.3017905963993996),\n",
       " (1018, 8.3026282110173373),\n",
       " (500, 8.3087358925455845),\n",
       " (240, 8.311005120075226),\n",
       " (18, 8.3135515249631649),\n",
       " (766, 8.3153833529354646),\n",
       " (553, 8.3169834166457406),\n",
       " (643, 8.3253847948614705),\n",
       " (363, 8.328980906951525),\n",
       " (39, 8.3335501457513281),\n",
       " (967, 8.334064527537171),\n",
       " (178, 8.3348475865785545),\n",
       " (579, 8.3406407567130074),\n",
       " (753, 8.3468142143427659),\n",
       " (220, 8.3482660883462572),\n",
       " (737, 8.3487554638730224),\n",
       " (856, 8.3491620915033486),\n",
       " (342, 8.3523765545701139),\n",
       " (783, 8.3541746027011303),\n",
       " (690, 8.354666797966793),\n",
       " (893, 8.3556790177747047),\n",
       " (129, 8.3657341080102405),\n",
       " (533, 8.3665682585718919),\n",
       " (946, 8.3668963458477563),\n",
       " (215, 8.3670251700731662),\n",
       " (832, 8.3702711016268943),\n",
       " (635, 8.3734883281569772),\n",
       " (911, 8.3736472053802071),\n",
       " (225, 8.3758665743577563),\n",
       " (529, 8.3806278115487309),\n",
       " (669, 8.3807448496954464),\n",
       " (739, 8.3960545259312056),\n",
       " (891, 8.4008422539157852),\n",
       " (236, 8.4171094503996446),\n",
       " (173, 8.4182427022860384),\n",
       " (996, 8.4218045953600082),\n",
       " (181, 8.4220337351383954),\n",
       " (574, 8.4237546241667101),\n",
       " (903, 8.4305749670506902),\n",
       " (63, 8.4338287097343425),\n",
       " (702, 8.4459819374006742),\n",
       " (223, 8.4461668047459639),\n",
       " (133, 8.4480261409138748),\n",
       " (723, 8.4508432521204835),\n",
       " (256, 8.4578235815785021),\n",
       " (596, 8.4717926411451678),\n",
       " (977, 8.4767989254827594),\n",
       " (354, 8.482046136466268),\n",
       " (534, 8.4832367357500136),\n",
       " (218, 8.4843180931947835),\n",
       " (618, 8.4855073707914546),\n",
       " (925, 8.4864871829550346),\n",
       " (333, 8.4898532922378926),\n",
       " (693, 8.4905995874860274),\n",
       " (550, 8.4916353535520859),\n",
       " (125, 8.4953205820855384),\n",
       " (230, 8.4959540436297001),\n",
       " (651, 8.5028338767726197),\n",
       " (307, 8.5033661313430216),\n",
       " (372, 8.5068643571076041),\n",
       " (520, 8.5150083697149199),\n",
       " (442, 8.5165953791844835),\n",
       " (210, 8.5175915744075876),\n",
       " (163, 8.5224231114146001),\n",
       " (716, 8.5349169873928954),\n",
       " (362, 8.5419731628458386),\n",
       " (301, 8.5430482772670189),\n",
       " (160, 8.5509425193698938),\n",
       " (1002, 8.5523859719030426),\n",
       " (664, 8.5531152267321779),\n",
       " (985, 8.5542724133058474),\n",
       " (501, 8.5552112556140258),\n",
       " (677, 8.5605037352526203),\n",
       " (435, 8.5629285667525004),\n",
       " (197, 8.5720278603970286),\n",
       " (504, 8.5745014249976954),\n",
       " (793, 8.5745597106408802),\n",
       " (14, 8.5778641944823786),\n",
       " (863, 8.5805336303542319),\n",
       " (144, 8.5825181700807764),\n",
       " (902, 8.5844470634568424),\n",
       " (288, 8.5880564005808253),\n",
       " (237, 8.5890364633318903),\n",
       " (224, 8.5933445981619059),\n",
       " (485, 8.5967009870600375),\n",
       " (244, 8.5993499011316192),\n",
       " (1005, 8.6010176834247432),\n",
       " (502, 8.6020550561203972),\n",
       " (211, 8.6024134497157156),\n",
       " (918, 8.6105095704296719),\n",
       " (421, 8.6117993003359885),\n",
       " (772, 8.6217317113472731),\n",
       " (694, 8.6219807871294147),\n",
       " (36, 8.6321974751091783),\n",
       " (448, 8.6336478577132514),\n",
       " (979, 8.638646807622365),\n",
       " (650, 8.6428127666912644),\n",
       " (932, 8.6437658097878298),\n",
       " (151, 8.6455805529642689),\n",
       " (347, 8.646375453776356),\n",
       " (745, 8.6508895079410077),\n",
       " (941, 8.6568570251149453),\n",
       " (60, 8.661651540737541),\n",
       " (332, 8.6630479230980466),\n",
       " (900, 8.6646820197318508),\n",
       " (554, 8.6709334106221618),\n",
       " (174, 8.6729089303882354),\n",
       " (43, 8.6763930710324608),\n",
       " (940, 8.6766402641748961),\n",
       " (457, 8.6815352788335307),\n",
       " (430, 8.683702665803418),\n",
       " (158, 8.6839962470705672),\n",
       " (1010, 8.6846696353806667),\n",
       " (563, 8.6869833343634379),\n",
       " (796, 8.7043781673126936),\n",
       " (817, 8.7076344352648434),\n",
       " (263, 8.7077434690747371),\n",
       " (658, 8.7165635846066838),\n",
       " (830, 8.7292464022009142),\n",
       " (992, 8.7362938406685533),\n",
       " (756, 8.7363347356463059),\n",
       " (481, 8.7391308060141579),\n",
       " (150, 8.7406769333702634),\n",
       " (708, 8.7481942599296083),\n",
       " (213, 8.7559922903092353),\n",
       " (972, 8.7640065108445935),\n",
       " (414, 8.764345597647333),\n",
       " (486, 8.7760340909412893),\n",
       " (1003, 8.7777198627798754),\n",
       " (399, 8.780087980891393),\n",
       " (20, 8.7814780572493003),\n",
       " (59, 8.7840584877324019),\n",
       " (666, 8.7860253279643228),\n",
       " (489, 8.7881850747283483),\n",
       " (246, 8.7965756740313452),\n",
       " (134, 8.7982349062787968),\n",
       " (919, 8.7991006589830238),\n",
       " (601, 8.8050737665482899),\n",
       " (313, 8.8130656126507514),\n",
       " (909, 8.814483610592756),\n",
       " (599, 8.8175399845764328),\n",
       " (530, 8.8215149566859203),\n",
       " (143, 8.833208893098881),\n",
       " (395, 8.8435090556569769),\n",
       " (189, 8.8440558022677287),\n",
       " (208, 8.8599970519582456),\n",
       " (1009, 8.8605628107969885),\n",
       " (545, 8.8748460773371374),\n",
       " (108, 8.887725653386795),\n",
       " (590, 8.8996515066556725),\n",
       " (357, 8.9048914268870973),\n",
       " (880, 8.9066595550520304),\n",
       " (775, 8.9129237104422661),\n",
       " (589, 8.9179704530205033),\n",
       " (119, 8.9184986829318742),\n",
       " (800, 8.9193267877265452),\n",
       " (270, 8.9514980556631869),\n",
       " (83, 8.959736059957816),\n",
       " (657, 8.9632489437301803),\n",
       " (537, 8.9716090411257667),\n",
       " (145, 8.9888531831018561),\n",
       " (295, 8.9910555523781728),\n",
       " (346, 8.9910620354922557),\n",
       " (149, 8.9969400103630175),\n",
       " (21, 9.0002777606916009),\n",
       " (607, 9.0036378306616651),\n",
       " (826, 9.0067744344404108),\n",
       " (865, 9.0197124786891543),\n",
       " (621, 9.0233400028836108),\n",
       " (231, 9.0240414509978724),\n",
       " (637, 9.0244079388676575),\n",
       " (944, 9.0248448145207707),\n",
       " (117, 9.0326082776533454),\n",
       " (202, 9.0328088757616882),\n",
       " (161, 9.0451205402993224),\n",
       " (70, 9.0570501685683471),\n",
       " (205, 9.0603852294645204),\n",
       " (278, 9.0626406770287176),\n",
       " (786, 9.067876450598348),\n",
       " (859, 9.0751879313056278),\n",
       " (458, 9.0815911566445866),\n",
       " (269, 9.0845936339292059),\n",
       " (53, 9.0932025033753412),\n",
       " (921, 9.0944929946351927),\n",
       " (715, 9.1085254426857265),\n",
       " (411, 9.1317340561536735),\n",
       " (106, 9.1351503050451406),\n",
       " (681, 9.1368725167066103),\n",
       " (993, 9.1706921225517348),\n",
       " (148, 9.1791723329972754),\n",
       " (274, 9.1864164033145475),\n",
       " (869, 9.1895635749223423),\n",
       " (279, 9.2090119661012935),\n",
       " (668, 9.2123411519613683),\n",
       " (931, 9.2909472336385655),\n",
       " (692, 9.292854278023114),\n",
       " (266, 9.2979045937227927),\n",
       " (686, 9.3036047784871929),\n",
       " (396, 9.3054319203800233),\n",
       " (879, 9.3054798216969505),\n",
       " (609, 9.3142671760516649),\n",
       " (447, 9.31505387436275),\n",
       " (576, 9.3415166806992573),\n",
       " (198, 9.3474737768242164),\n",
       " (569, 9.3819282030657494),\n",
       " (91, 9.3922331706635287),\n",
       " (419, 9.394560877033884),\n",
       " (179, 9.3988589527406798),\n",
       " (773, 9.4155973218833697),\n",
       " (427, 9.4203677254431959),\n",
       " (750, 9.4216501550076668),\n",
       " (252, 9.429711559637747),\n",
       " (620, 9.4418204502562961),\n",
       " (706, 9.4578731974811738),\n",
       " (532, 9.4627026828721164),\n",
       " (656, 9.4772840537216396),\n",
       " (459, 9.4884088907659105),\n",
       " (536, 9.4893215645325153),\n",
       " (901, 9.4939012183986939),\n",
       " (958, 9.5013133547868804),\n",
       " (853, 9.5041308097024082),\n",
       " (120, 9.5044975163331191),\n",
       " (493, 9.5143210902044579),\n",
       " (241, 9.5321661498179875),\n",
       " (583, 9.5551721615894643),\n",
       " (1012, 9.5568062422543925),\n",
       " (474, 9.566739239843594),\n",
       " (250, 9.5677884940974973),\n",
       " (41, 9.5844433721375992),\n",
       " (523, 9.6011648157908027),\n",
       " (526, 9.6364414711367683),\n",
       " (356, 9.6377922024362821),\n",
       " (428, 9.6434418935619046),\n",
       " (797, 9.6672979824330483),\n",
       " (611, 9.6953434517447974),\n",
       " (369, 9.6956854940076376),\n",
       " (587, 9.7093832730638567),\n",
       " (652, 9.7756588930611965),\n",
       " (557, 9.7801782753267332),\n",
       " (192, 9.7950600775907066),\n",
       " (142, 9.828333990155345),\n",
       " (228, 9.8471502741760872),\n",
       " (349, 9.8556439946100696),\n",
       " (659, 9.8606872221734641),\n",
       " (1000, 9.8709909901022908),\n",
       " (110, 9.8775069197426646),\n",
       " (798, 9.9249437093378798),\n",
       " (98, 9.964184315736837),\n",
       " (951, 9.9663405483165448),\n",
       " (452, 10.040752676516057),\n",
       " (852, 10.060148914000044),\n",
       " (200, 10.060427227245096),\n",
       " (135, 10.062227910049396),\n",
       " (492, 10.107382872925999),\n",
       " (671, 10.110770698848016),\n",
       " (3, 10.130782354908595),\n",
       " (544, 10.160070488220892),\n",
       " (998, 10.178033229357437),\n",
       " (385, 10.180521305055285),\n",
       " (206, 10.189263998956394),\n",
       " (373, 10.191637261385408),\n",
       " (441, 10.229469512562014),\n",
       " (94, 10.317445759658339),\n",
       " (678, 10.330346620599054),\n",
       " (878, 10.405325513078832),\n",
       " (857, 10.456658128362747),\n",
       " (126, 10.472261215671287),\n",
       " (966, 10.475800606940274),\n",
       " (490, 10.494309200300439),\n",
       " (642, 10.502276373161122),\n",
       " (272, 10.506008053629845),\n",
       " (335, 10.516688161168611),\n",
       " (13, 10.525690238525121),\n",
       " (440, 10.552488429310401),\n",
       " (552, 10.614998306572577),\n",
       " (303, 10.69706092634973),\n",
       " (453, 10.729143635174383),\n",
       " (602, 10.737365652253013),\n",
       " ...]"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dct.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0286100527181\n"
     ]
    }
   ],
   "source": [
    "pred_2 = np.zeros((438, 1))\n",
    "\n",
    "cluster_size = 6\n",
    "\n",
    "for ii in range(features_test.shape[0]):\n",
    "    \n",
    "    dct = {}\n",
    "    \n",
    "    for iii in range(features_train.shape[0]):\n",
    "        dct[iii] = np.linalg.norm(features_train[iii, :] - features_test[ii, :])\n",
    "        \n",
    "    dd = sorted(dct.items(), key=lambda x: x[1])[:cluster_size]\n",
    "    \n",
    "    counter = 0.0\n",
    "    \n",
    "    for i in range(cluster_size):\n",
    "        counter = counter + labels_train[ dd[i][0] ]\n",
    "    \n",
    "    pred_2[ii] = counter / cluster_size\n",
    "    \n",
    "print(mean_absolute_error(labels_test, pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0273277097972\n",
      "{'reduce_dim__n_components': 265}\n",
      "Wall time: 20min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbr = GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classify', gbr)\n",
    "])\n",
    "\n",
    "parameters_grid = {\n",
    "        'reduce_dim__n_components': list(range(10, 625, 5)),\n",
    "        #'reduce_dim__n_components': [600],\n",
    "    }\n",
    "    \n",
    "gcv = GridSearchCV(pipe, parameters_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "gcv.fit(features, labels.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0206620912125\n",
      "{'n_estimators': 600}\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbr = GradientBoostingRegressor(min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "\n",
    "parameters_grid = {\n",
    "        'n_estimators': [250, 300, 350, 400, 450, 500, 550, 600],\n",
    "    }\n",
    "    \n",
    "gcv = GridSearchCV(gbr, parameters_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "gcv.fit(features, labels.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###    6      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For KRG best fit to train data is 0.004414931048544484 at 99\n",
      "For LGB best fit to train data is 0.003320830661547149 at 98\n",
      "For MLP best fit to train data is 0.020899522036716148 at 87\n",
      "For GBR best fit to train data is 0.006609260722143669 at 99\n",
      "For XGB best fit to train data is 0.007482646769804072 at 97\n",
      "For RF best fit to train data is 0.006544310230829068 at 99\n",
      "Wall time: 21min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf1 = GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "clf2 = MLPRegressor(hidden_layer_sizes=(150, 50, 10), random_state=23)\n",
    "clf3 = lgb.LGBMRegressor(n_estimators=250, max_depth=5, learning_rate=0.075, random_state=23, n_jobs=-1)\n",
    "clf4 = RandomForestRegressor(n_estimators=350, min_samples_leaf=2, min_samples_split=2, n_jobs=-1, random_state=23)\n",
    "clf5 = KernelRidge(kernel='rbf', alpha=0.01, gamma=0.0046415888336127772)\n",
    "clf6 = xgboost.XGBRegressor(max_depth=3, n_estimators=350)\n",
    "\n",
    "clfs = {\n",
    "    (clf1, \"GBR\"),\n",
    "    (clf2, \"MLP\"),\n",
    "    (clf3, \"LGB\"),\n",
    "    (clf4, \"RF\"),\n",
    "    (clf5, \"KRG\"),\n",
    "    (clf6, \"XGB\")\n",
    "}\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results[\"Target\"] = labels_test.reshape(-1)\n",
    "\n",
    "MAX_DEALS_TO_DROP = 100\n",
    "\n",
    "for item in clfs:\n",
    "    clf = item[0]\n",
    "    clf_name = item[1]\n",
    "\n",
    "    tmp_f = np.array(features_train)\n",
    "    tmp_l = np.array(labels_train)\n",
    "    \n",
    "    tmp_scores = []\n",
    "    tmp_predictions = pd.DataFrame()\n",
    "\n",
    "    for i in range(MAX_DEALS_TO_DROP):\n",
    "        clf.fit(tmp_f, tmp_l.reshape(-1))\n",
    "\n",
    "        prediction_train = clf.predict(tmp_f).reshape(-1, 1)\n",
    "        tmp_scores.append(mean_absolute_error(tmp_l, prediction_train))\n",
    "        \n",
    "        tmp1 = np.abs(tmp_l - prediction_train)\n",
    "        d = tmp1.argmax()\n",
    "\n",
    "        tmp_f = np.delete(tmp_f, d, 0)\n",
    "        tmp_l = np.delete(tmp_l, d, 0)\n",
    "        \n",
    "        prediction_test = clf.predict(features_test).reshape(-1)\n",
    "        tmp_predictions[clf_name + str(i)] = prediction_test\n",
    "    \n",
    "    idx = argmin(tmp_scores)\n",
    "    \n",
    "    print(\"For {} best fit to train data is {} at {}\".format(clf_name, min(tmp_scores), idx))\n",
    "    \n",
    "    results[clf_name + str(idx)] = tmp_predictions[clf_name + str(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>KRG99</th>\n",
       "      <th>LGB98</th>\n",
       "      <th>GBR99</th>\n",
       "      <th>XGB97</th>\n",
       "      <th>RF99</th>\n",
       "      <th>GBM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.246358</td>\n",
       "      <td>0.215891</td>\n",
       "      <td>0.220931</td>\n",
       "      <td>0.217291</td>\n",
       "      <td>0.207699</td>\n",
       "      <td>0.233758</td>\n",
       "      <td>0.208321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284106</td>\n",
       "      <td>0.250369</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.241411</td>\n",
       "      <td>0.233683</td>\n",
       "      <td>0.252862</td>\n",
       "      <td>0.228792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.234437</td>\n",
       "      <td>0.201216</td>\n",
       "      <td>0.216678</td>\n",
       "      <td>0.232361</td>\n",
       "      <td>0.223333</td>\n",
       "      <td>0.211115</td>\n",
       "      <td>0.230963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.245033</td>\n",
       "      <td>0.231642</td>\n",
       "      <td>0.243733</td>\n",
       "      <td>0.231486</td>\n",
       "      <td>0.248910</td>\n",
       "      <td>0.210354</td>\n",
       "      <td>0.220751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.079470</td>\n",
       "      <td>0.130078</td>\n",
       "      <td>0.163670</td>\n",
       "      <td>0.148244</td>\n",
       "      <td>0.154188</td>\n",
       "      <td>0.183551</td>\n",
       "      <td>0.142222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.296026</td>\n",
       "      <td>0.299347</td>\n",
       "      <td>0.290720</td>\n",
       "      <td>0.272892</td>\n",
       "      <td>0.280960</td>\n",
       "      <td>0.289386</td>\n",
       "      <td>0.275710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.151984</td>\n",
       "      <td>0.138162</td>\n",
       "      <td>0.142027</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.139402</td>\n",
       "      <td>0.130217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.579012</td>\n",
       "      <td>0.548356</td>\n",
       "      <td>0.582324</td>\n",
       "      <td>0.573224</td>\n",
       "      <td>0.638096</td>\n",
       "      <td>0.523348</td>\n",
       "      <td>0.630222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.178808</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.177298</td>\n",
       "      <td>0.177105</td>\n",
       "      <td>0.174363</td>\n",
       "      <td>0.175416</td>\n",
       "      <td>0.174033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.582781</td>\n",
       "      <td>0.446261</td>\n",
       "      <td>0.486610</td>\n",
       "      <td>0.473377</td>\n",
       "      <td>0.472188</td>\n",
       "      <td>0.411903</td>\n",
       "      <td>0.486946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.242252</td>\n",
       "      <td>0.251899</td>\n",
       "      <td>0.243732</td>\n",
       "      <td>0.245245</td>\n",
       "      <td>0.251033</td>\n",
       "      <td>0.239219</td>\n",
       "      <td>0.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.274834</td>\n",
       "      <td>0.294174</td>\n",
       "      <td>0.297695</td>\n",
       "      <td>0.291601</td>\n",
       "      <td>0.277977</td>\n",
       "      <td>0.294973</td>\n",
       "      <td>0.293802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.341060</td>\n",
       "      <td>0.322565</td>\n",
       "      <td>0.328869</td>\n",
       "      <td>0.286498</td>\n",
       "      <td>0.290925</td>\n",
       "      <td>0.282316</td>\n",
       "      <td>0.311810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.217881</td>\n",
       "      <td>0.209953</td>\n",
       "      <td>0.219730</td>\n",
       "      <td>0.218414</td>\n",
       "      <td>0.218279</td>\n",
       "      <td>0.200899</td>\n",
       "      <td>0.219237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.160265</td>\n",
       "      <td>0.116760</td>\n",
       "      <td>0.144941</td>\n",
       "      <td>0.149489</td>\n",
       "      <td>0.153597</td>\n",
       "      <td>0.133764</td>\n",
       "      <td>0.146305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.046225</td>\n",
       "      <td>0.084116</td>\n",
       "      <td>0.090883</td>\n",
       "      <td>0.073224</td>\n",
       "      <td>0.088393</td>\n",
       "      <td>0.105307</td>\n",
       "      <td>0.079787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.192053</td>\n",
       "      <td>0.205746</td>\n",
       "      <td>0.208250</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.199755</td>\n",
       "      <td>0.199851</td>\n",
       "      <td>0.196559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.231788</td>\n",
       "      <td>0.215768</td>\n",
       "      <td>0.202965</td>\n",
       "      <td>0.223973</td>\n",
       "      <td>0.210515</td>\n",
       "      <td>0.210349</td>\n",
       "      <td>0.206451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.105828</td>\n",
       "      <td>0.149670</td>\n",
       "      <td>0.154295</td>\n",
       "      <td>0.153107</td>\n",
       "      <td>0.160250</td>\n",
       "      <td>0.154081</td>\n",
       "      <td>0.156256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.156954</td>\n",
       "      <td>0.166010</td>\n",
       "      <td>0.183113</td>\n",
       "      <td>0.186087</td>\n",
       "      <td>0.172193</td>\n",
       "      <td>0.180937</td>\n",
       "      <td>0.178442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.393377</td>\n",
       "      <td>0.365628</td>\n",
       "      <td>0.417281</td>\n",
       "      <td>0.388174</td>\n",
       "      <td>0.375503</td>\n",
       "      <td>0.358106</td>\n",
       "      <td>0.371661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.184106</td>\n",
       "      <td>0.178926</td>\n",
       "      <td>0.178704</td>\n",
       "      <td>0.182912</td>\n",
       "      <td>0.185011</td>\n",
       "      <td>0.181063</td>\n",
       "      <td>0.179092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.224091</td>\n",
       "      <td>0.210663</td>\n",
       "      <td>0.208704</td>\n",
       "      <td>0.220475</td>\n",
       "      <td>0.218459</td>\n",
       "      <td>0.211326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.434437</td>\n",
       "      <td>0.343291</td>\n",
       "      <td>0.371125</td>\n",
       "      <td>0.342486</td>\n",
       "      <td>0.338412</td>\n",
       "      <td>0.322768</td>\n",
       "      <td>0.348764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.165563</td>\n",
       "      <td>0.150607</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.177637</td>\n",
       "      <td>0.172963</td>\n",
       "      <td>0.175419</td>\n",
       "      <td>0.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.105960</td>\n",
       "      <td>0.100589</td>\n",
       "      <td>0.094141</td>\n",
       "      <td>0.109253</td>\n",
       "      <td>0.116535</td>\n",
       "      <td>0.109515</td>\n",
       "      <td>0.114546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.113907</td>\n",
       "      <td>0.127560</td>\n",
       "      <td>0.123263</td>\n",
       "      <td>0.142983</td>\n",
       "      <td>0.139233</td>\n",
       "      <td>0.134067</td>\n",
       "      <td>0.136787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.170199</td>\n",
       "      <td>0.176390</td>\n",
       "      <td>0.159867</td>\n",
       "      <td>0.164210</td>\n",
       "      <td>0.166216</td>\n",
       "      <td>0.163435</td>\n",
       "      <td>0.169977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.189404</td>\n",
       "      <td>0.206439</td>\n",
       "      <td>0.177105</td>\n",
       "      <td>0.180404</td>\n",
       "      <td>0.185568</td>\n",
       "      <td>0.191025</td>\n",
       "      <td>0.188084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.219205</td>\n",
       "      <td>0.212554</td>\n",
       "      <td>0.200861</td>\n",
       "      <td>0.208788</td>\n",
       "      <td>0.210800</td>\n",
       "      <td>0.207448</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.145033</td>\n",
       "      <td>0.138360</td>\n",
       "      <td>0.143009</td>\n",
       "      <td>0.121392</td>\n",
       "      <td>0.124958</td>\n",
       "      <td>0.139017</td>\n",
       "      <td>0.125795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.489934</td>\n",
       "      <td>0.390619</td>\n",
       "      <td>0.491710</td>\n",
       "      <td>0.462976</td>\n",
       "      <td>0.479550</td>\n",
       "      <td>0.406771</td>\n",
       "      <td>0.482956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.124503</td>\n",
       "      <td>0.136494</td>\n",
       "      <td>0.132620</td>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.109486</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.115528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.198013</td>\n",
       "      <td>0.212515</td>\n",
       "      <td>0.206132</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.204775</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.231698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.282768</td>\n",
       "      <td>0.262165</td>\n",
       "      <td>0.247390</td>\n",
       "      <td>0.259563</td>\n",
       "      <td>0.238377</td>\n",
       "      <td>0.249018</td>\n",
       "      <td>0.256095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.334673</td>\n",
       "      <td>0.326888</td>\n",
       "      <td>0.300370</td>\n",
       "      <td>0.299952</td>\n",
       "      <td>0.296712</td>\n",
       "      <td>0.290298</td>\n",
       "      <td>0.315469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.249007</td>\n",
       "      <td>0.204834</td>\n",
       "      <td>0.191393</td>\n",
       "      <td>0.202741</td>\n",
       "      <td>0.198415</td>\n",
       "      <td>0.186931</td>\n",
       "      <td>0.193031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.296689</td>\n",
       "      <td>0.275795</td>\n",
       "      <td>0.269012</td>\n",
       "      <td>0.272741</td>\n",
       "      <td>0.266997</td>\n",
       "      <td>0.238007</td>\n",
       "      <td>0.289637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.213907</td>\n",
       "      <td>0.225348</td>\n",
       "      <td>0.212924</td>\n",
       "      <td>0.216972</td>\n",
       "      <td>0.218064</td>\n",
       "      <td>0.217258</td>\n",
       "      <td>0.212606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.205298</td>\n",
       "      <td>0.165223</td>\n",
       "      <td>0.200598</td>\n",
       "      <td>0.201032</td>\n",
       "      <td>0.197598</td>\n",
       "      <td>0.216522</td>\n",
       "      <td>0.196515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target     KRG99     LGB98     GBR99     XGB97      RF99       GBM\n",
       "0   0.246358  0.215891  0.220931  0.217291  0.207699  0.233758  0.208321\n",
       "1   0.284106  0.250369  0.243277  0.241411  0.233683  0.252862  0.228792\n",
       "2   0.234437  0.201216  0.216678  0.232361  0.223333  0.211115  0.230963\n",
       "3   0.245033  0.231642  0.243733  0.231486  0.248910  0.210354  0.220751\n",
       "4   0.079470  0.130078  0.163670  0.148244  0.154188  0.183551  0.142222\n",
       "5   0.296026  0.299347  0.290720  0.272892  0.280960  0.289386  0.275710\n",
       "6   0.085430  0.151984  0.138162  0.142027  0.141186  0.139402  0.130217\n",
       "7   0.579012  0.548356  0.582324  0.573224  0.638096  0.523348  0.630222\n",
       "8   0.178808  0.180147  0.177298  0.177105  0.174363  0.175416  0.174033\n",
       "9   0.582781  0.446261  0.486610  0.473377  0.472188  0.411903  0.486946\n",
       "10  0.242252  0.251899  0.243732  0.245245  0.251033  0.239219  0.261600\n",
       "11  0.274834  0.294174  0.297695  0.291601  0.277977  0.294973  0.293802\n",
       "12  0.341060  0.322565  0.328869  0.286498  0.290925  0.282316  0.311810\n",
       "13  0.217881  0.209953  0.219730  0.218414  0.218279  0.200899  0.219237\n",
       "14  0.160265  0.116760  0.144941  0.149489  0.153597  0.133764  0.146305\n",
       "15  0.046225  0.084116  0.090883  0.073224  0.088393  0.105307  0.079787\n",
       "16  0.192053  0.205746  0.208250  0.196721  0.199755  0.199851  0.196559\n",
       "17  0.231788  0.215768  0.202965  0.223973  0.210515  0.210349  0.206451\n",
       "18  0.105828  0.149670  0.154295  0.153107  0.160250  0.154081  0.156256\n",
       "19  0.156954  0.166010  0.183113  0.186087  0.172193  0.180937  0.178442\n",
       "20  0.393377  0.365628  0.417281  0.388174  0.375503  0.358106  0.371661\n",
       "21  0.184106  0.178926  0.178704  0.182912  0.185011  0.181063  0.179092\n",
       "22  0.211921  0.224091  0.210663  0.208704  0.220475  0.218459  0.211326\n",
       "23  0.434437  0.343291  0.371125  0.342486  0.338412  0.322768  0.348764\n",
       "24  0.165563  0.150607  0.178990  0.177637  0.172963  0.175419  0.180100\n",
       "25  0.105960  0.100589  0.094141  0.109253  0.116535  0.109515  0.114546\n",
       "26  0.113907  0.127560  0.123263  0.142983  0.139233  0.134067  0.136787\n",
       "27  0.170199  0.176390  0.159867  0.164210  0.166216  0.163435  0.169977\n",
       "28  0.189404  0.206439  0.177105  0.180404  0.185568  0.191025  0.188084\n",
       "29  0.219205  0.212554  0.200861  0.208788  0.210800  0.207448  0.204000\n",
       "30  0.145033  0.138360  0.143009  0.121392  0.124958  0.139017  0.125795\n",
       "31  0.489934  0.390619  0.491710  0.462976  0.479550  0.406771  0.482956\n",
       "32  0.124503  0.136494  0.132620  0.109502  0.109486  0.127601  0.115528\n",
       "33  0.198013  0.212515  0.206132  0.220900  0.204775  0.235933  0.231698\n",
       "34  0.282768  0.262165  0.247390  0.259563  0.238377  0.249018  0.256095\n",
       "35  0.334673  0.326888  0.300370  0.299952  0.296712  0.290298  0.315469\n",
       "36  0.249007  0.204834  0.191393  0.202741  0.198415  0.186931  0.193031\n",
       "37  0.296689  0.275795  0.269012  0.272741  0.266997  0.238007  0.289637\n",
       "38  0.213907  0.225348  0.212924  0.216972  0.218064  0.217258  0.212606\n",
       "39  0.205298  0.165223  0.200598  0.201032  0.197598  0.216522  0.196515"
      ]
     },
     "execution_count": 1347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del results[\"MLP87\"]\n",
    "results.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0198273191639\n"
     ]
    }
   ],
   "source": [
    "average = (results[\"XGB97\"] + results[\"LGB98\"] + results[\"GBR99\"]) / 3.0\n",
    "\n",
    "print(\"Average vs Target\", mean_absolute_error(results.Target, average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>KRG99</th>\n",
       "      <th>LGB98</th>\n",
       "      <th>GBR99</th>\n",
       "      <th>XGB97</th>\n",
       "      <th>RF99</th>\n",
       "      <th>GBM</th>\n",
       "      <th>LGB</th>\n",
       "      <th>K_Ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.246358</td>\n",
       "      <td>0.215891</td>\n",
       "      <td>0.220931</td>\n",
       "      <td>0.217291</td>\n",
       "      <td>0.207699</td>\n",
       "      <td>0.233758</td>\n",
       "      <td>0.211308</td>\n",
       "      <td>0.218134</td>\n",
       "      <td>0.206438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284106</td>\n",
       "      <td>0.250369</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.241411</td>\n",
       "      <td>0.233683</td>\n",
       "      <td>0.252862</td>\n",
       "      <td>0.231720</td>\n",
       "      <td>0.230721</td>\n",
       "      <td>0.200416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.234437</td>\n",
       "      <td>0.201216</td>\n",
       "      <td>0.216678</td>\n",
       "      <td>0.232361</td>\n",
       "      <td>0.223333</td>\n",
       "      <td>0.211115</td>\n",
       "      <td>0.212378</td>\n",
       "      <td>0.204213</td>\n",
       "      <td>0.199507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.245033</td>\n",
       "      <td>0.231642</td>\n",
       "      <td>0.243733</td>\n",
       "      <td>0.231486</td>\n",
       "      <td>0.248910</td>\n",
       "      <td>0.210354</td>\n",
       "      <td>0.221657</td>\n",
       "      <td>0.244341</td>\n",
       "      <td>0.214670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.079470</td>\n",
       "      <td>0.130078</td>\n",
       "      <td>0.163670</td>\n",
       "      <td>0.148244</td>\n",
       "      <td>0.154188</td>\n",
       "      <td>0.183551</td>\n",
       "      <td>0.142103</td>\n",
       "      <td>0.145544</td>\n",
       "      <td>0.123910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.296026</td>\n",
       "      <td>0.299347</td>\n",
       "      <td>0.290720</td>\n",
       "      <td>0.272892</td>\n",
       "      <td>0.280960</td>\n",
       "      <td>0.289386</td>\n",
       "      <td>0.272607</td>\n",
       "      <td>0.275519</td>\n",
       "      <td>0.286952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.151984</td>\n",
       "      <td>0.138162</td>\n",
       "      <td>0.142027</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.139402</td>\n",
       "      <td>0.127285</td>\n",
       "      <td>0.144077</td>\n",
       "      <td>0.142821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.579012</td>\n",
       "      <td>0.548356</td>\n",
       "      <td>0.582324</td>\n",
       "      <td>0.573224</td>\n",
       "      <td>0.638096</td>\n",
       "      <td>0.523348</td>\n",
       "      <td>0.656476</td>\n",
       "      <td>0.583510</td>\n",
       "      <td>0.567856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.178808</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.177298</td>\n",
       "      <td>0.177105</td>\n",
       "      <td>0.174363</td>\n",
       "      <td>0.175416</td>\n",
       "      <td>0.171330</td>\n",
       "      <td>0.182721</td>\n",
       "      <td>0.178252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.582781</td>\n",
       "      <td>0.446261</td>\n",
       "      <td>0.486610</td>\n",
       "      <td>0.473377</td>\n",
       "      <td>0.472188</td>\n",
       "      <td>0.411903</td>\n",
       "      <td>0.490589</td>\n",
       "      <td>0.485691</td>\n",
       "      <td>0.458840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.242252</td>\n",
       "      <td>0.251899</td>\n",
       "      <td>0.243732</td>\n",
       "      <td>0.245245</td>\n",
       "      <td>0.251033</td>\n",
       "      <td>0.239219</td>\n",
       "      <td>0.260679</td>\n",
       "      <td>0.248522</td>\n",
       "      <td>0.256499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.274834</td>\n",
       "      <td>0.294174</td>\n",
       "      <td>0.297695</td>\n",
       "      <td>0.291601</td>\n",
       "      <td>0.277977</td>\n",
       "      <td>0.294973</td>\n",
       "      <td>0.278257</td>\n",
       "      <td>0.310484</td>\n",
       "      <td>0.311272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.341060</td>\n",
       "      <td>0.322565</td>\n",
       "      <td>0.328869</td>\n",
       "      <td>0.286498</td>\n",
       "      <td>0.290925</td>\n",
       "      <td>0.282316</td>\n",
       "      <td>0.303079</td>\n",
       "      <td>0.320803</td>\n",
       "      <td>0.332369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.217881</td>\n",
       "      <td>0.209953</td>\n",
       "      <td>0.219730</td>\n",
       "      <td>0.218414</td>\n",
       "      <td>0.218279</td>\n",
       "      <td>0.200899</td>\n",
       "      <td>0.207602</td>\n",
       "      <td>0.230308</td>\n",
       "      <td>0.198061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.160265</td>\n",
       "      <td>0.116760</td>\n",
       "      <td>0.144941</td>\n",
       "      <td>0.149489</td>\n",
       "      <td>0.153597</td>\n",
       "      <td>0.133764</td>\n",
       "      <td>0.141577</td>\n",
       "      <td>0.146638</td>\n",
       "      <td>0.115141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.046225</td>\n",
       "      <td>0.084116</td>\n",
       "      <td>0.090883</td>\n",
       "      <td>0.073224</td>\n",
       "      <td>0.088393</td>\n",
       "      <td>0.105307</td>\n",
       "      <td>0.082286</td>\n",
       "      <td>0.092494</td>\n",
       "      <td>0.090698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.192053</td>\n",
       "      <td>0.205746</td>\n",
       "      <td>0.208250</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.199755</td>\n",
       "      <td>0.199851</td>\n",
       "      <td>0.203427</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.216638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.231788</td>\n",
       "      <td>0.215768</td>\n",
       "      <td>0.202965</td>\n",
       "      <td>0.223973</td>\n",
       "      <td>0.210515</td>\n",
       "      <td>0.210349</td>\n",
       "      <td>0.212290</td>\n",
       "      <td>0.195628</td>\n",
       "      <td>0.223713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.105828</td>\n",
       "      <td>0.149670</td>\n",
       "      <td>0.154295</td>\n",
       "      <td>0.153107</td>\n",
       "      <td>0.160250</td>\n",
       "      <td>0.154081</td>\n",
       "      <td>0.157724</td>\n",
       "      <td>0.142711</td>\n",
       "      <td>0.131166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.156954</td>\n",
       "      <td>0.166010</td>\n",
       "      <td>0.183113</td>\n",
       "      <td>0.186087</td>\n",
       "      <td>0.172193</td>\n",
       "      <td>0.180937</td>\n",
       "      <td>0.178918</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>0.164390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.393377</td>\n",
       "      <td>0.365628</td>\n",
       "      <td>0.417281</td>\n",
       "      <td>0.388174</td>\n",
       "      <td>0.375503</td>\n",
       "      <td>0.358106</td>\n",
       "      <td>0.397048</td>\n",
       "      <td>0.387801</td>\n",
       "      <td>0.376788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.184106</td>\n",
       "      <td>0.178926</td>\n",
       "      <td>0.178704</td>\n",
       "      <td>0.182912</td>\n",
       "      <td>0.185011</td>\n",
       "      <td>0.181063</td>\n",
       "      <td>0.186598</td>\n",
       "      <td>0.174099</td>\n",
       "      <td>0.178525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.224091</td>\n",
       "      <td>0.210663</td>\n",
       "      <td>0.208704</td>\n",
       "      <td>0.220475</td>\n",
       "      <td>0.218459</td>\n",
       "      <td>0.213310</td>\n",
       "      <td>0.206399</td>\n",
       "      <td>0.210249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.434437</td>\n",
       "      <td>0.343291</td>\n",
       "      <td>0.371125</td>\n",
       "      <td>0.342486</td>\n",
       "      <td>0.338412</td>\n",
       "      <td>0.322768</td>\n",
       "      <td>0.350313</td>\n",
       "      <td>0.354142</td>\n",
       "      <td>0.349692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.165563</td>\n",
       "      <td>0.150607</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.177637</td>\n",
       "      <td>0.172963</td>\n",
       "      <td>0.175419</td>\n",
       "      <td>0.176620</td>\n",
       "      <td>0.166956</td>\n",
       "      <td>0.145764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.105960</td>\n",
       "      <td>0.100589</td>\n",
       "      <td>0.094141</td>\n",
       "      <td>0.109253</td>\n",
       "      <td>0.116535</td>\n",
       "      <td>0.109515</td>\n",
       "      <td>0.123491</td>\n",
       "      <td>0.109898</td>\n",
       "      <td>0.116849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.113907</td>\n",
       "      <td>0.127560</td>\n",
       "      <td>0.123263</td>\n",
       "      <td>0.142983</td>\n",
       "      <td>0.139233</td>\n",
       "      <td>0.134067</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.138996</td>\n",
       "      <td>0.119247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.170199</td>\n",
       "      <td>0.176390</td>\n",
       "      <td>0.159867</td>\n",
       "      <td>0.164210</td>\n",
       "      <td>0.166216</td>\n",
       "      <td>0.163435</td>\n",
       "      <td>0.169030</td>\n",
       "      <td>0.162670</td>\n",
       "      <td>0.177506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.189404</td>\n",
       "      <td>0.206439</td>\n",
       "      <td>0.177105</td>\n",
       "      <td>0.180404</td>\n",
       "      <td>0.185568</td>\n",
       "      <td>0.191025</td>\n",
       "      <td>0.188632</td>\n",
       "      <td>0.185165</td>\n",
       "      <td>0.207960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.219205</td>\n",
       "      <td>0.212554</td>\n",
       "      <td>0.200861</td>\n",
       "      <td>0.208788</td>\n",
       "      <td>0.210800</td>\n",
       "      <td>0.207448</td>\n",
       "      <td>0.210167</td>\n",
       "      <td>0.228663</td>\n",
       "      <td>0.232317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.145033</td>\n",
       "      <td>0.138360</td>\n",
       "      <td>0.143009</td>\n",
       "      <td>0.121392</td>\n",
       "      <td>0.124958</td>\n",
       "      <td>0.139017</td>\n",
       "      <td>0.123127</td>\n",
       "      <td>0.130237</td>\n",
       "      <td>0.126351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.489934</td>\n",
       "      <td>0.390619</td>\n",
       "      <td>0.491710</td>\n",
       "      <td>0.462976</td>\n",
       "      <td>0.479550</td>\n",
       "      <td>0.406771</td>\n",
       "      <td>0.481896</td>\n",
       "      <td>0.472235</td>\n",
       "      <td>0.421686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.124503</td>\n",
       "      <td>0.136494</td>\n",
       "      <td>0.132620</td>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.109486</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.111632</td>\n",
       "      <td>0.119401</td>\n",
       "      <td>0.129992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.198013</td>\n",
       "      <td>0.212515</td>\n",
       "      <td>0.206132</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.204775</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.208047</td>\n",
       "      <td>0.209744</td>\n",
       "      <td>0.188067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.282768</td>\n",
       "      <td>0.262165</td>\n",
       "      <td>0.247390</td>\n",
       "      <td>0.259563</td>\n",
       "      <td>0.238377</td>\n",
       "      <td>0.249018</td>\n",
       "      <td>0.256723</td>\n",
       "      <td>0.261377</td>\n",
       "      <td>0.255543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.334673</td>\n",
       "      <td>0.326888</td>\n",
       "      <td>0.300370</td>\n",
       "      <td>0.299952</td>\n",
       "      <td>0.296712</td>\n",
       "      <td>0.290298</td>\n",
       "      <td>0.300942</td>\n",
       "      <td>0.304256</td>\n",
       "      <td>0.334925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.249007</td>\n",
       "      <td>0.204834</td>\n",
       "      <td>0.191393</td>\n",
       "      <td>0.202741</td>\n",
       "      <td>0.198415</td>\n",
       "      <td>0.186931</td>\n",
       "      <td>0.199233</td>\n",
       "      <td>0.201131</td>\n",
       "      <td>0.191253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.296689</td>\n",
       "      <td>0.275795</td>\n",
       "      <td>0.269012</td>\n",
       "      <td>0.272741</td>\n",
       "      <td>0.266997</td>\n",
       "      <td>0.238007</td>\n",
       "      <td>0.263797</td>\n",
       "      <td>0.293188</td>\n",
       "      <td>0.251410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.213907</td>\n",
       "      <td>0.225348</td>\n",
       "      <td>0.212924</td>\n",
       "      <td>0.216972</td>\n",
       "      <td>0.218064</td>\n",
       "      <td>0.217258</td>\n",
       "      <td>0.203893</td>\n",
       "      <td>0.218219</td>\n",
       "      <td>0.231653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.205298</td>\n",
       "      <td>0.165223</td>\n",
       "      <td>0.200598</td>\n",
       "      <td>0.201032</td>\n",
       "      <td>0.197598</td>\n",
       "      <td>0.216522</td>\n",
       "      <td>0.187473</td>\n",
       "      <td>0.201216</td>\n",
       "      <td>0.153621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.221854</td>\n",
       "      <td>0.237317</td>\n",
       "      <td>0.258728</td>\n",
       "      <td>0.254798</td>\n",
       "      <td>0.239120</td>\n",
       "      <td>0.221680</td>\n",
       "      <td>0.275863</td>\n",
       "      <td>0.287366</td>\n",
       "      <td>0.206219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.180132</td>\n",
       "      <td>0.167525</td>\n",
       "      <td>0.183122</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.178279</td>\n",
       "      <td>0.191684</td>\n",
       "      <td>0.179817</td>\n",
       "      <td>0.178675</td>\n",
       "      <td>0.154625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.320530</td>\n",
       "      <td>0.319492</td>\n",
       "      <td>0.270642</td>\n",
       "      <td>0.264329</td>\n",
       "      <td>0.274363</td>\n",
       "      <td>0.240658</td>\n",
       "      <td>0.291148</td>\n",
       "      <td>0.288165</td>\n",
       "      <td>0.302252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.260265</td>\n",
       "      <td>0.246286</td>\n",
       "      <td>0.248452</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.247075</td>\n",
       "      <td>0.255742</td>\n",
       "      <td>0.242701</td>\n",
       "      <td>0.241798</td>\n",
       "      <td>0.257113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.205166</td>\n",
       "      <td>0.189936</td>\n",
       "      <td>0.196102</td>\n",
       "      <td>0.192702</td>\n",
       "      <td>0.187858</td>\n",
       "      <td>0.168164</td>\n",
       "      <td>0.177801</td>\n",
       "      <td>0.177557</td>\n",
       "      <td>0.185321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.205298</td>\n",
       "      <td>0.205077</td>\n",
       "      <td>0.210021</td>\n",
       "      <td>0.223395</td>\n",
       "      <td>0.220152</td>\n",
       "      <td>0.230989</td>\n",
       "      <td>0.202245</td>\n",
       "      <td>0.217960</td>\n",
       "      <td>0.199036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.181457</td>\n",
       "      <td>0.186672</td>\n",
       "      <td>0.181826</td>\n",
       "      <td>0.179288</td>\n",
       "      <td>0.179574</td>\n",
       "      <td>0.171210</td>\n",
       "      <td>0.187054</td>\n",
       "      <td>0.180032</td>\n",
       "      <td>0.178220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.180132</td>\n",
       "      <td>0.260395</td>\n",
       "      <td>0.247220</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.223508</td>\n",
       "      <td>0.297110</td>\n",
       "      <td>0.216641</td>\n",
       "      <td>0.228115</td>\n",
       "      <td>0.237052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.285033</td>\n",
       "      <td>0.305669</td>\n",
       "      <td>0.242387</td>\n",
       "      <td>0.253898</td>\n",
       "      <td>0.235696</td>\n",
       "      <td>0.254808</td>\n",
       "      <td>0.260858</td>\n",
       "      <td>0.255197</td>\n",
       "      <td>0.324723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.370861</td>\n",
       "      <td>0.304866</td>\n",
       "      <td>0.291022</td>\n",
       "      <td>0.302448</td>\n",
       "      <td>0.307858</td>\n",
       "      <td>0.285385</td>\n",
       "      <td>0.285021</td>\n",
       "      <td>0.273434</td>\n",
       "      <td>0.321377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target     KRG99     LGB98     GBR99     XGB97      RF99       GBM  \\\n",
       "0   0.246358  0.215891  0.220931  0.217291  0.207699  0.233758  0.211308   \n",
       "1   0.284106  0.250369  0.243277  0.241411  0.233683  0.252862  0.231720   \n",
       "2   0.234437  0.201216  0.216678  0.232361  0.223333  0.211115  0.212378   \n",
       "3   0.245033  0.231642  0.243733  0.231486  0.248910  0.210354  0.221657   \n",
       "4   0.079470  0.130078  0.163670  0.148244  0.154188  0.183551  0.142103   \n",
       "5   0.296026  0.299347  0.290720  0.272892  0.280960  0.289386  0.272607   \n",
       "6   0.085430  0.151984  0.138162  0.142027  0.141186  0.139402  0.127285   \n",
       "7   0.579012  0.548356  0.582324  0.573224  0.638096  0.523348  0.656476   \n",
       "8   0.178808  0.180147  0.177298  0.177105  0.174363  0.175416  0.171330   \n",
       "9   0.582781  0.446261  0.486610  0.473377  0.472188  0.411903  0.490589   \n",
       "10  0.242252  0.251899  0.243732  0.245245  0.251033  0.239219  0.260679   \n",
       "11  0.274834  0.294174  0.297695  0.291601  0.277977  0.294973  0.278257   \n",
       "12  0.341060  0.322565  0.328869  0.286498  0.290925  0.282316  0.303079   \n",
       "13  0.217881  0.209953  0.219730  0.218414  0.218279  0.200899  0.207602   \n",
       "14  0.160265  0.116760  0.144941  0.149489  0.153597  0.133764  0.141577   \n",
       "15  0.046225  0.084116  0.090883  0.073224  0.088393  0.105307  0.082286   \n",
       "16  0.192053  0.205746  0.208250  0.196721  0.199755  0.199851  0.203427   \n",
       "17  0.231788  0.215768  0.202965  0.223973  0.210515  0.210349  0.212290   \n",
       "18  0.105828  0.149670  0.154295  0.153107  0.160250  0.154081  0.157724   \n",
       "19  0.156954  0.166010  0.183113  0.186087  0.172193  0.180937  0.178918   \n",
       "20  0.393377  0.365628  0.417281  0.388174  0.375503  0.358106  0.397048   \n",
       "21  0.184106  0.178926  0.178704  0.182912  0.185011  0.181063  0.186598   \n",
       "22  0.211921  0.224091  0.210663  0.208704  0.220475  0.218459  0.213310   \n",
       "23  0.434437  0.343291  0.371125  0.342486  0.338412  0.322768  0.350313   \n",
       "24  0.165563  0.150607  0.178990  0.177637  0.172963  0.175419  0.176620   \n",
       "25  0.105960  0.100589  0.094141  0.109253  0.116535  0.109515  0.123491   \n",
       "26  0.113907  0.127560  0.123263  0.142983  0.139233  0.134067  0.139357   \n",
       "27  0.170199  0.176390  0.159867  0.164210  0.166216  0.163435  0.169030   \n",
       "28  0.189404  0.206439  0.177105  0.180404  0.185568  0.191025  0.188632   \n",
       "29  0.219205  0.212554  0.200861  0.208788  0.210800  0.207448  0.210167   \n",
       "30  0.145033  0.138360  0.143009  0.121392  0.124958  0.139017  0.123127   \n",
       "31  0.489934  0.390619  0.491710  0.462976  0.479550  0.406771  0.481896   \n",
       "32  0.124503  0.136494  0.132620  0.109502  0.109486  0.127601  0.111632   \n",
       "33  0.198013  0.212515  0.206132  0.220900  0.204775  0.235933  0.208047   \n",
       "34  0.282768  0.262165  0.247390  0.259563  0.238377  0.249018  0.256723   \n",
       "35  0.334673  0.326888  0.300370  0.299952  0.296712  0.290298  0.300942   \n",
       "36  0.249007  0.204834  0.191393  0.202741  0.198415  0.186931  0.199233   \n",
       "37  0.296689  0.275795  0.269012  0.272741  0.266997  0.238007  0.263797   \n",
       "38  0.213907  0.225348  0.212924  0.216972  0.218064  0.217258  0.203893   \n",
       "39  0.205298  0.165223  0.200598  0.201032  0.197598  0.216522  0.187473   \n",
       "40  0.221854  0.237317  0.258728  0.254798  0.239120  0.221680  0.275863   \n",
       "41  0.180132  0.167525  0.183122  0.169900  0.178279  0.191684  0.179817   \n",
       "42  0.320530  0.319492  0.270642  0.264329  0.274363  0.240658  0.291148   \n",
       "43  0.260265  0.246286  0.248452  0.249668  0.247075  0.255742  0.242701   \n",
       "44  0.205166  0.189936  0.196102  0.192702  0.187858  0.168164  0.177801   \n",
       "45  0.205298  0.205077  0.210021  0.223395  0.220152  0.230989  0.202245   \n",
       "46  0.181457  0.186672  0.181826  0.179288  0.179574  0.171210  0.187054   \n",
       "47  0.180132  0.260395  0.247220  0.228975  0.223508  0.297110  0.216641   \n",
       "48  0.285033  0.305669  0.242387  0.253898  0.235696  0.254808  0.260858   \n",
       "49  0.370861  0.304866  0.291022  0.302448  0.307858  0.285385  0.285021   \n",
       "\n",
       "         LGB   K_Ridge  \n",
       "0   0.218134  0.206438  \n",
       "1   0.230721  0.200416  \n",
       "2   0.204213  0.199507  \n",
       "3   0.244341  0.214670  \n",
       "4   0.145544  0.123910  \n",
       "5   0.275519  0.286952  \n",
       "6   0.144077  0.142821  \n",
       "7   0.583510  0.567856  \n",
       "8   0.182721  0.178252  \n",
       "9   0.485691  0.458840  \n",
       "10  0.248522  0.256499  \n",
       "11  0.310484  0.311272  \n",
       "12  0.320803  0.332369  \n",
       "13  0.230308  0.198061  \n",
       "14  0.146638  0.115141  \n",
       "15  0.092494  0.090698  \n",
       "16  0.196279  0.216638  \n",
       "17  0.195628  0.223713  \n",
       "18  0.142711  0.131166  \n",
       "19  0.179117  0.164390  \n",
       "20  0.387801  0.376788  \n",
       "21  0.174099  0.178525  \n",
       "22  0.206399  0.210249  \n",
       "23  0.354142  0.349692  \n",
       "24  0.166956  0.145764  \n",
       "25  0.109898  0.116849  \n",
       "26  0.138996  0.119247  \n",
       "27  0.162670  0.177506  \n",
       "28  0.185165  0.207960  \n",
       "29  0.228663  0.232317  \n",
       "30  0.130237  0.126351  \n",
       "31  0.472235  0.421686  \n",
       "32  0.119401  0.129992  \n",
       "33  0.209744  0.188067  \n",
       "34  0.261377  0.255543  \n",
       "35  0.304256  0.334925  \n",
       "36  0.201131  0.191253  \n",
       "37  0.293188  0.251410  \n",
       "38  0.218219  0.231653  \n",
       "39  0.201216  0.153621  \n",
       "40  0.287366  0.206219  \n",
       "41  0.178675  0.154625  \n",
       "42  0.288165  0.302252  \n",
       "43  0.241798  0.257113  \n",
       "44  0.177557  0.185321  \n",
       "45  0.217960  0.199036  \n",
       "46  0.180032  0.178220  \n",
       "47  0.228115  0.237052  \n",
       "48  0.255197  0.324723  \n",
       "49  0.273434  0.321377  "
      ]
     },
     "execution_count": 1694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering 1: handling features mostly correlated with target  \n",
    "  ,    ,  :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[544, 564, 567, 606]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_1_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson r for col 544 is 0.59507792562\n",
      "Pearson r for col 564 is 0.59725533954\n",
      "Pearson r for col 567 is 0.69953753481\n",
      "Pearson r for col 606 is 0.622064164667\n",
      "New features shape is (1022, 679)\n"
     ]
    }
   ],
   "source": [
    "if (use_fe_1):\n",
    "    for col in fe_1_cols:\n",
    "        print(\"Pearson r for col\", col, \"is\", pearsonr(features_train[:, col].reshape(-1, 1), labels_train.reshape(-1, 1))[0][0])\n",
    "        \n",
    "        # Train DT\n",
    "        DTR = DecisionTreeRegressor(random_state=23)\n",
    "        DTR.fit(features_train[:, col].reshape(-1, 1), labels_train)\n",
    "\n",
    "        DTR_prediction1 = DTR.predict(features_train[:, col].reshape(-1, 1))\n",
    "        DTR_prediction2 = DTR.predict(features_test[:, col].reshape(-1, 1))\n",
    "\n",
    "        features_train = np.concatenate( [features_train, DTR_prediction1.reshape(-1, 1)], axis=1)\n",
    "        features_test = np.concatenate( [features_test, DTR_prediction2.reshape(-1, 1)], axis=1)\n",
    "        \n",
    "print(\"New features shape is\", features_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2707,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_mutual_info(X, y):\n",
    "    return mutual_info_regression(X, y, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2708,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of features: 10 and the error: 0.0281051605919\n",
      "Num of features: 15 and the error: 0.026373524992\n",
      "Num of features: 20 and the error: 0.0252935764034\n",
      "Num of features: 25 and the error: 0.0228834465052\n",
      "Num of features: 30 and the error: 0.0228548795961\n",
      "Num of features: 35 and the error: 0.0220050909192\n",
      "Num of features: 40 and the error: 0.0214715726029\n",
      "Num of features: 45 and the error: 0.0217378356126\n",
      "Num of features: 50 and the error: 0.021290717931\n",
      "Num of features: 55 and the error: 0.0219111232816\n",
      "Num of features: 60 and the error: 0.0215221542437\n",
      "Num of features: 65 and the error: 0.0215535434889\n",
      "Num of features: 70 and the error: 0.0212812832387\n",
      "Num of features: 75 and the error: 0.0215981896786\n",
      "Num of features: 80 and the error: 0.0211061671218\n",
      "Num of features: 85 and the error: 0.021094033704\n",
      "Num of features: 90 and the error: 0.0210766351066\n",
      "Num of features: 95 and the error: 0.0208754660513\n",
      "Num of features: 100 and the error: 0.0205909384025\n",
      "Num of features: 105 and the error: 0.0209059489799\n",
      "Num of features: 110 and the error: 0.0208799496859\n",
      "Num of features: 115 and the error: 0.0207155668077\n",
      "Num of features: 120 and the error: 0.0209898222987\n",
      "Num of features: 125 and the error: 0.0209105526823\n",
      "Num of features: 130 and the error: 0.0210519507616\n",
      "Num of features: 135 and the error: 0.0210717315209\n",
      "Num of features: 140 and the error: 0.020735309131\n",
      "Num of features: 145 and the error: 0.0207022035816\n",
      "Num of features: 150 and the error: 0.0203413113779\n",
      "Num of features: 155 and the error: 0.0199961119705\n",
      "Num of features: 160 and the error: 0.0203482967191\n",
      "Num of features: 165 and the error: 0.0203872342714\n",
      "Num of features: 170 and the error: 0.0202536874717\n",
      "Num of features: 175 and the error: 0.0204024357144\n",
      "Num of features: 180 and the error: 0.020112147645\n",
      "Num of features: 185 and the error: 0.020411272357\n",
      "Num of features: 190 and the error: 0.0203289210848\n",
      "Num of features: 195 and the error: 0.0205768348593\n",
      "Num of features: 200 and the error: 0.0205235980566\n",
      "Num of features: 205 and the error: 0.0202710681765\n",
      "Num of features: 210 and the error: 0.0203062104941\n",
      "Num of features: 215 and the error: 0.0202932052856\n",
      "Num of features: 220 and the error: 0.0201437087995\n",
      "Num of features: 225 and the error: 0.0204833294148\n",
      "Num of features: 230 and the error: 0.0204815237622\n",
      "Num of features: 235 and the error: 0.020387384137\n",
      "Num of features: 240 and the error: 0.020238793555\n",
      "Num of features: 245 and the error: 0.0201333050936\n",
      "Num of features: 250 and the error: 0.0201463319623\n",
      "Num of features: 255 and the error: 0.0200700326401\n",
      "Num of features: 260 and the error: 0.0201297532748\n",
      "Num of features: 265 and the error: 0.0202327295\n",
      "Num of features: 270 and the error: 0.0204352562782\n",
      "Num of features: 275 and the error: 0.0205997059423\n",
      "Num of features: 280 and the error: 0.0206139482989\n",
      "Num of features: 285 and the error: 0.0206605037438\n",
      "Num of features: 290 and the error: 0.0207696528226\n",
      "Num of features: 295 and the error: 0.0207484927685\n",
      "Num of features: 300 and the error: 0.0207610082254\n",
      "Wall time: 15min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for num in list(range(10, 301, 5)):\n",
    "    sel = SelectKBest(score_func=my_mutual_info, k=num)\n",
    "    sel.fit(features_train, labels_train.reshape(-1))\n",
    "    \n",
    "    best_features_train = sel.transform(features_train)\n",
    "    best_features_test = sel.transform(features_test)\n",
    "    \n",
    "    clf = GradientBoostingRegressor(n_estimators=250, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "    #clf = xgboost.XGBRegressor(max_depth=4, n_estimators=300)\n",
    "    \n",
    "    clf.fit(best_features_train, labels_train.reshape(-1))\n",
    "\n",
    "    prediction = clf.predict(best_features_test)\n",
    "\n",
    "    print(\"Num of features:\", num, \"and the error:\", mean_absolute_error(labels_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   GBM = 0.01999  155   \n",
    "XGB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2709,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel = SelectKBest(score_func=my_mutual_info, k=155)\n",
    "sel.fit(features_train, labels_train.reshape(-1))\n",
    "\n",
    "best_features_train = sel.transform(features_train)\n",
    "best_features_test = sel.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2710,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['Target'] = labels_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0234871402589\n",
      "{'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 350}\n",
      "0.0202941592297\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = choose_GBM(best_features_train, labels_train)\n",
    "\n",
    "clf.fit(best_features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(best_features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0199961119705\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingRegressor(n_estimators=250, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "clf.fit(best_features_train, labels_train.reshape(-1))\n",
    "\n",
    "prediction = clf.predict(best_features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['GBM'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0240306834833\n",
      "{'max_depth': 4, 'n_estimators': 300}\n",
      "0.0210248258735\n"
     ]
    }
   ],
   "source": [
    "clf = choose_XGB(best_features_train, labels_train)\n",
    "\n",
    "clf.fit(best_features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(best_features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['XGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2713,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 5.2198795484543084e-21\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 5.219879548454294e-21\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0247435408483\n",
      "{'alpha': 1.0, 'gamma': 0.021544346900318832, 'kernel': 'polynomial'}\n",
      "0.0249165079801\n"
     ]
    }
   ],
   "source": [
    "clf = choose_KernelRidge(best_features_train, labels_train)\n",
    "\n",
    "clf.fit(best_features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(best_features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results['K_Ridge'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0200525379468\n",
      "GBM vs Target 0.0199961119705\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(results[['GBM', 'K_Ridge', 'XGB']], axis=1)\n",
    "\n",
    "print(\"Average vs Target\", mean_absolute_error(results.Target, average))\n",
    "print(\"GBM vs Target\", mean_absolute_error(results.Target, results.GBM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering 4: use Cluster # as additional feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if (False):\n",
    "    for num_of_clusters in range(2, 32):\n",
    "        k_means = KMeans(n_clusters=num_of_clusters, random_state=23, n_jobs=-1,)\n",
    "        k_means.fit(features_train)\n",
    "\n",
    "        prediction_train = k_means.predict(features_train)\n",
    "        prediction_test = k_means.predict(features_test)\n",
    "\n",
    "        features_train_cluster = np.concatenate((features_train, prediction_train.reshape(-1, 1)), axis=1)\n",
    "        features_test_cluster = np.concatenate((features_test, prediction_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        clf = GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "        #clf = xgboost.XGBRegressor(max_depth=3, n_estimators=350)\n",
    "        clf.fit(features_train_cluster, labels_train.reshape(-1))\n",
    "\n",
    "        prediction = clf.predict(features_test_cluster)\n",
    "\n",
    "        print(\"Number of clusters:\", num_of_clusters, \", and the error is\", mean_absolute_error(labels_test, prediction))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (False):\n",
    "    for num_of_clusters in range(2, 32):\n",
    "        k_means = KMeans(n_clusters=num_of_clusters, random_state=23, n_jobs=-1)\n",
    "        k_means.fit(features)\n",
    "        prediction = k_means.predict(features)\n",
    "\n",
    "        features_cluster = np.concatenate((features, prediction.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        f_train, f_test, l_train, l_test = train_test_split(features_cluster, labels, test_size=0.3, random_state=23)\n",
    "\n",
    "        clf = GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "        clf.fit(f_train, l_train.reshape(-1))\n",
    "        prediction = clf.predict(f_test)\n",
    "\n",
    "        print(\"Number of clusters:\", num_of_clusters, \", and the error is\", mean_absolute_error(l_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (use_fe_4):\n",
    "    k_means = KMeans(n_clusters=22, random_state=23, n_jobs=-1)\n",
    "    k_means.fit(features)\n",
    "    prediction = k_means.predict(features)\n",
    "    features = np.concatenate((features, prediction.reshape(-1, 1)), axis=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
