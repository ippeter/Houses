{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression, LinearRegression, SGDRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Helpers\n",
    "#\n",
    "def write_to_submission_file(predicted_labels, sale_ids, out_file=\"submission.csv\", target='SalePrice', index_label=\"Id\"):\n",
    "    \n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = sale_ids,\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n"
     ]
    }
   ],
   "source": [
    "# Load House train & test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Put the labels aside\n",
    "labels_orig = train.SalePrice.as_matrix().reshape(-1, 1)\n",
    "\n",
    "# This will be needed after pre-processing\n",
    "size_of_train = train.shape[0]\n",
    "print(size_of_train)\n",
    "\n",
    "# Merge datasets\n",
    "del train['SalePrice']\n",
    "\n",
    "train = pd.concat([train, test])\n",
    "\n",
    "# Define variables\n",
    "cols = []\n",
    "cols_count = 0\n",
    "\n",
    "# Handling years as categorial \n",
    "use_fe_2 = False\n",
    "\n",
    "# Using kind of a total sum of square feet\n",
    "use_fe_3 = True\n",
    "\n",
    "# Using new feature of Clustering\n",
    "use_fe_4 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSSubClass\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "features = enc.fit_transform(train.MSSubClass.values.reshape(-1, 1))\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MSZoning\n",
    "#train.MSZoning.fillna(\"RM\", inplace=True)\n",
    "train.MSZoning.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.MSZoning).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotFrontage\n",
    "train.LotFrontage.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.LotFrontage.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotArea\n",
    "features = np.concatenate( [features, train.LotArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Street\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Street).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Alley\n",
    "train.Alley.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Alley).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotShape\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LotShape).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LandContour\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LandContour).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "#train.Utilities.fillna(\"AllPub\", inplace=True)\n",
    "train.Utilities.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Utilities).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LotConfig\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LotConfig).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LandSlope\n",
    "features = np.concatenate( [features, pd.get_dummies(train.LandSlope).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Neighborhood \n",
    "features = np.concatenate( [features, pd.get_dummies(train.Neighborhood).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Condition1\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Condition1).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Condition2\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Condition2).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BldgType\n",
    "features = np.concatenate( [features, pd.get_dummies(train.BldgType).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# HouseStyle\n",
    "features = np.concatenate( [features, pd.get_dummies(train.HouseStyle).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# OverallQual\n",
    "#enc = OneHotEncoder(sparse=False)\n",
    "#features = np.concatenate( [features, enc.fit_transform(train.OverallQual.values.reshape(-1, 1))], axis=1 )\n",
    "features = np.concatenate( [features, train.OverallQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# OverallCond\n",
    "#enc = OneHotEncoder(sparse=False)\n",
    "#features = np.concatenate( [features, enc.fit_transform(train.OverallCond.values.reshape(-1, 1))], axis=1 )\n",
    "features = np.concatenate( [features, train.OverallCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YearBuilt\n",
    "train['HouseAge'] = train.YrSold - train.YearBuilt\n",
    "features = np.concatenate( [features, train.HouseAge.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# YearRemodAdd\n",
    "train['AgeSinceRemod'] = train.YrSold - train.YearRemodAdd\n",
    "features = np.concatenate( [features, train.AgeSinceRemod.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# RoofStyle\n",
    "features = np.concatenate( [features, pd.get_dummies(train.RoofStyle).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# RoofMatl\n",
    "features = np.concatenate( [features, pd.get_dummies(train.RoofMatl).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Exterior1st\n",
    "#train.Exterior1st.fillna(\"Wd Sdng\", inplace=True)\n",
    "train.Exterior1st.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Exterior1st).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Exterior2nd\n",
    "#train.Exterior2nd.fillna(\"Wd Sdng\", inplace=True)\n",
    "train.Exterior2nd.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Exterior2nd).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MasVnrType\n",
    "#train.MasVnrType.fillna(\"None\", inplace=True)\n",
    "train.MasVnrType.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.MasVnrType).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MasVnrArea\n",
    "train.MasVnrArea.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.MasVnrArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# ExterQual\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0}\n",
    "train[\"ExterQual\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.ExterQual).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.ExterQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# ExterCond\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0}\n",
    "train[\"ExterCond\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.ExterCond).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.ExterCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Foundation\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Foundation).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtQual\n",
    "train.BsmtQual.fillna(\"NA\", inplace=True)\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtQual\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtQual).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtCond\n",
    "train.BsmtCond.fillna(\"NA\", inplace=True)\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtCond\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtCond).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtExposure\n",
    "train.BsmtExposure.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.BsmtExposure).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinType1\n",
    "train.BsmtFinType1.fillna(\"NA\", inplace=True)\n",
    "di = {\"GLQ\": 6.0, \"ALQ\": 5.0, \"BLQ\": 4.0, \"Rec\": 3.0, \"LwQ\": 2.0, \"Unf\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtFinType1\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtFinType1).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtFinType1.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinSF1\n",
    "train.BsmtFinSF1.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtFinSF1.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinType2\n",
    "train.BsmtFinType2.fillna(\"NA\", inplace=True)\n",
    "di = {\"GLQ\": 6.0, \"ALQ\": 5.0, \"BLQ\": 4.0, \"Rec\": 3.0, \"LwQ\": 2.0, \"Unf\": 1.0, \"NA\": 0.0}\n",
    "train[\"BsmtFinType2\"].replace(di, inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.BsmtFinType2).as_matrix()], axis=1 )\n",
    "features = np.concatenate( [features, train.BsmtFinType2.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFinSF2\n",
    "train.BsmtFinSF2.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtFinSF2.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtUnfSF\n",
    "train.BsmtUnfSF.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtUnfSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# TotalBsmtSF\n",
    "train.TotalBsmtSF.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.TotalBsmtSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Heating\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Heating).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# HeatingQC\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.HeatingQC).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0}\n",
    "train[\"HeatingQC\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.HeatingQC.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# CentralAir\n",
    "features = np.concatenate( [features, pd.get_dummies(train.CentralAir).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Electrical\n",
    "train.Electrical.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Electrical).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# 1stFlrSF\n",
    "features = np.concatenate( [features, train['1stFlrSF'].as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# 2ndFlrSF\n",
    "features = np.concatenate( [features, train['2ndFlrSF'].as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# LowQualFinSF\n",
    "features = np.concatenate( [features, train.LowQualFinSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GrLivArea\n",
    "features = np.concatenate( [features, train.GrLivArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtFullBath\n",
    "train.BsmtFullBath.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtFullBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BsmtHalfBath\n",
    "train.BsmtHalfBath.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.BsmtHalfBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FullBath\n",
    "features = np.concatenate( [features, train.FullBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# HalfBath\n",
    "features = np.concatenate( [features, train.HalfBath.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# BedroomAbvGr\n",
    "features = np.concatenate( [features, train.BedroomAbvGr.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# KitchenAbvGr\n",
    "features = np.concatenate( [features, train.KitchenAbvGr.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# KitchenQual\n",
    "#train.KitchenQual.fillna(\"TA\", inplace=True)\n",
    "train.KitchenQual.fillna(\"Unknown\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.KitchenQual).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"Unknown\": 0.0}\n",
    "train[\"KitchenQual\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.KitchenQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# TotRmsAbvGrd\n",
    "features = np.concatenate( [features, train.TotRmsAbvGrd.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Functional\n",
    "#train.Functional.fillna(\"Typ\", inplace=True)\n",
    "train.Functional.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Functional).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Fireplaces\n",
    "features = np.concatenate( [features, train.Fireplaces.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# FireplaceQu\n",
    "train.FireplaceQu.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.FireplaceQu).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageType\n",
    "train.GarageType.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.GarageType).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GarageYrBlt\n",
    "train.GarageYrBlt.fillna(train.YearBuilt, inplace=True)\n",
    "train['GarageAge'] = train.YrSold - train.GarageYrBlt\n",
    "features = np.concatenate( [features, train.GarageAge.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageFinish\n",
    "train.GarageFinish.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.GarageFinish).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageCars\n",
    "train.GarageCars.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageCars.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageArea\n",
    "train.GarageArea.fillna(0.0, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageQual\n",
    "train.GarageQual.fillna(\"NA\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.GarageQual).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"GarageQual\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageQual.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# GarageCond\n",
    "train.GarageCond.fillna(\"NA\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.GarageCond).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"GarageCond\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.GarageCond.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# PavedDrive\n",
    "features = np.concatenate( [features, pd.get_dummies(train.PavedDrive).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# WoodDeckSF\n",
    "features = np.concatenate( [features, train.WoodDeckSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# OpenPorchSF\n",
    "features = np.concatenate( [features, train.OpenPorchSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# EnclosedPorch\n",
    "features = np.concatenate( [features, train.EnclosedPorch.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3SsnPorch\n",
    "features = np.concatenate( [features, train['3SsnPorch'].as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# ScreenPorch\n",
    "features = np.concatenate( [features, train.ScreenPorch.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# PoolArea\n",
    "features = np.concatenate( [features, train.PoolArea.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# PoolQC\n",
    "train.PoolQC.fillna(\"NA\", inplace=True)\n",
    "#features = np.concatenate( [features, pd.get_dummies(train.PoolQC).as_matrix()], axis=1 )\n",
    "di = {\"Ex\": 5.0, \"Gd\": 4.0, \"TA\": 3.0, \"Fa\": 2.0, \"Po\": 1.0, \"NA\": 0.0}\n",
    "train[\"PoolQC\"].replace(di, inplace=True)\n",
    "features = np.concatenate( [features, train.PoolQC.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# Fence\n",
    "train.Fence.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.Fence).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MiscFeature\n",
    "train.MiscFeature.fillna(\"NA\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.MiscFeature).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# MiscVal\n",
    "#features = np.concatenate( [features, train.MiscVal.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "#cols_count = cols_count + 1\n",
    "\n",
    "# MoSold\n",
    "if (use_fe_2):\n",
    "    features = np.concatenate( [features, pd.get_dummies(train.MoSold).as_matrix()], axis=1 )\n",
    "else:\n",
    "    features = np.concatenate( [features, train.MoSold.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    \n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# YrSold\n",
    "if (use_fe_2):\n",
    "    features = np.concatenate( [features, pd.get_dummies(train.YrSold).as_matrix()], axis=1 )\n",
    "else:\n",
    "    features = np.concatenate( [features, train.YrSold.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    \n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# SaleType\n",
    "#train.SaleType.fillna(\"WD\", inplace=True)\n",
    "train.SaleType.fillna(\"Unknown\", inplace=True)\n",
    "features = np.concatenate( [features, pd.get_dummies(train.SaleType).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1\n",
    "\n",
    "# SaleCondition\n",
    "features = np.concatenate( [features, pd.get_dummies(train.SaleCondition).as_matrix()], axis=1 )\n",
    "cols_count = cols_count + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LivingAreaSF\n",
    "if (use_fe_3):\n",
    "    train['LivingAreaSF'] = train['1stFlrSF'] + train['2ndFlrSF'] + train['TotalBsmtSF'] + \\\n",
    "                            train['GarageArea'] + train['MasVnrArea'] + train['WoodDeckSF'] + \\\n",
    "                            train['OpenPorchSF'] + train['3SsnPorch'] + train['ScreenPorch']\n",
    "\n",
    "    features = np.concatenate( [features, train.LivingAreaSF.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    cols_count = cols_count + 1\n",
    "    \n",
    "    train['LandRatio'] = train['LivingAreaSF'] / train['LotArea']\n",
    "    features = np.concatenate( [features, train.LandRatio.as_matrix().reshape(-1, 1)], axis=1 )\n",
    "    cols_count = cols_count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 23 features scaled\n"
     ]
    }
   ],
   "source": [
    "# Prepare labels\n",
    "#labels = (train.SalePrice - train.MiscVal).values.reshape(-1, 1)\n",
    "labels = labels_orig\n",
    "\n",
    "# First scale labels\n",
    "labels = labels.astype(float)\n",
    "labels_max = labels.max()\n",
    "\n",
    "labels = labels / labels_max\n",
    "\n",
    "count = 0\n",
    "    \n",
    "for jj in range(features.shape[1]):\n",
    "    if((features[:, jj] > 25.).sum() > 0):\n",
    "        mx = float(features[:, jj].max())\n",
    "\n",
    "        features[:, jj] = features[:, jj] / mx\n",
    "        count = count + 1\n",
    "        \n",
    "print(\"Total\", count, \"features scaled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка наборов для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features processed: 80\n",
      "\n",
      "(1460, 279)\n",
      "(1460, 1)\n",
      "(1459, 279)\n"
     ]
    }
   ],
   "source": [
    "# Split for train and test sets\n",
    "features_train = features[:size_of_train, :]\n",
    "labels_train = labels\n",
    "features_test = features[size_of_train:, :]\n",
    "\n",
    "print(\"Total features processed:\", cols_count)\n",
    "print(\"\")\n",
    "print(features_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# For Averaging\n",
    "#\n",
    "results = pd.DataFrame()\n",
    "results_ada = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0205232781156\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def choose_LGB(X, y):\n",
    "    LGB = lgb.LGBMRegressor(random_state=23, n_jobs=-1)\n",
    "\n",
    "    parameters_grid = {\n",
    "        #\"boosting_type\": [\"gbdt\", \"dart\", \"goss\", \"rf\"],\n",
    "        \"n_estimators\": [50, 100, 150, 200, 250, 300, 350],\n",
    "        \"max_depth\": [2, 3, 4, 5, 6],\n",
    "        \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1],\n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(LGB, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_    \n",
    "\n",
    "\n",
    "clf = choose_LGB(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "results['LGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=300, learning_rate=0.5, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "results_ada['LGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_GBM(X, y):\n",
    "    GBR = GradientBoostingRegressor(random_state=23)\n",
    "\n",
    "    parameters_grid = {\n",
    "        \"n_estimators\": [250, 300, 350],\n",
    "        \"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "        \"min_samples_leaf\": [1, 2, 3, 4]\n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(GBR, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0202453183667\n",
      "{'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 350}\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = choose_GBM(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "results['GBM'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=300, learning_rate=0.5, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "results_ada['GBM'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00786508173164 42\n",
      "0.0105238355887\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = []\n",
    "\n",
    "tmp_f = np.array(features)\n",
    "tmp_l = np.array(labels)\n",
    "\n",
    "for i in range(75):\n",
    "    clf = GradientBoostingRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=2, random_state=23)\n",
    "    clf.fit(tmp_f, tmp_l.reshape(-1))\n",
    "    prediction = clf.predict(tmp_f).reshape(-1, 1)\n",
    "\n",
    "    res.append(mean_absolute_error(tmp_l, prediction))\n",
    "    \n",
    "    if (i == 42):\n",
    "        break\n",
    "\n",
    "    tmp1 = np.abs(tmp_l - prediction)\n",
    "    d = tmp1.argmax()\n",
    "\n",
    "    tmp_f = np.delete(tmp_f, d, 0)\n",
    "    tmp_l = np.delete(tmp_l, d, 0)\n",
    "    \n",
    "print(min(res), argmin(res))\n",
    "\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0174589900129\n"
     ]
    }
   ],
   "source": [
    "f_train, f_test, l_train, l_test = train_test_split(tmp_f, tmp_l, test_size=0.3, random_state=23)\n",
    "\n",
    "clf.fit(f_train, l_train.reshape(-1))\n",
    "prediction = clf.predict(f_test)\n",
    "\n",
    "print(mean_absolute_error(l_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023161170611\n",
      "{'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 400}\n",
      "Wall time: 12min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RFR = RandomForestRegressor(n_jobs=-1, random_state=23)\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": [200, 250, 300, 350, 400],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(RFR, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "gcv.fit(features_train, labels_train.reshape(-1))\n",
    "\n",
    "print(gcv.best_score_)\n",
    "print(gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gcv.best_estimator_\n",
    "#clf = RandomForestRegressor(n_estimators=350, min_samples_leaf=2, min_samples_split=2, n_jobs=-1, random_state=23)\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "results['RF'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "clf = gcv.best_estimator_\n",
    "\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=350, learning_rate=1.0, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "results_ada['RF'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_KernelRidge(X, y):\n",
    "    clf = KernelRidge()\n",
    "\n",
    "    parameters_grid = {\n",
    "        \"kernel\": ['polynomial', 'rbf'], \n",
    "        \"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "        \"gamma\": np.logspace(-3, 3, 10)    \n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(clf, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0226676633569\n",
      "{'alpha': 0.1, 'gamma': 0.0046415888336127772, 'kernel': 'polynomial'}\n"
     ]
    }
   ],
   "source": [
    "clf = choose_KernelRidge(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "results['KRG'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_ada['KRG'] = results['KRG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_XGB(X, y):\n",
    "    XGB = xgboost.XGBRegressor()\n",
    "\n",
    "    parameters_grid = {\n",
    "        #\"n_estimators\": [50, 100, 150, 200, 250, 300],\n",
    "        \"n_estimators\": [250, 300, 350],\n",
    "        #\"max_depth\": [2, 3, 4, 5, 6, 7, 8],\n",
    "        \"max_depth\": [3, 4, 5],\n",
    "        #\"learning_rate\": [0.1, 0.05, 0.01]\n",
    "        #\"learning_rate\": [0.1],\n",
    "    }\n",
    "\n",
    "    gcv = GridSearchCV(XGB, parameters_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gcv.fit(X, y.reshape(-1))\n",
    "\n",
    "    print(gcv.best_score_)\n",
    "    print(gcv.best_params_)\n",
    "    \n",
    "    return gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0203782092953\n",
      "{'max_depth': 3, 'n_estimators': 350}\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = choose_XGB(features_train, labels_train)\n",
    "\n",
    "clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "results['XGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#\n",
    "# AdaBoost\n",
    "#\n",
    "adb_clf = AdaBoostRegressor(base_estimator=clf, n_estimators=300, learning_rate=0.5, random_state=23)\n",
    "adb_clf.fit(features_train, labels_train.reshape(-1))\n",
    "prediction = adb_clf.predict(features_test)\n",
    "\n",
    "print(mean_absolute_error(labels_test, prediction))\n",
    "\n",
    "results_ada['XGB'] = prediction.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_ada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vs Target 0.0190728184582\n"
     ]
    }
   ],
   "source": [
    "tmp = 0.2*results_ada['KRG'] + 0.2*results_ada['RF'] + 0.1*results_ada['LGB'] + 0.0*results_ada['GBM'] + 0.5*results_ada['XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Submission\n",
    "#\n",
    "write_to_submission_file(tmp * labels_max, test.Id, out_file=\"submission.Ada+WA.csv\", target='SalePrice', index_label=\"Id\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
